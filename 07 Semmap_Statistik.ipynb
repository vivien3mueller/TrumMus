{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6ae3d4",
   "metadata": {},
   "source": [
    "SEMMAP - Erstellung zweier Keyword Visualisierungen für TRUMP und MUSK"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d53c754e",
   "metadata": {},
   "source": [
    "Semmap:\n",
    "An Semmap füttern: TSV-Datei mit einem Assotiationsmaß (und verschiedene zum vergleichen nehmen)\n",
    "Weg: Koordinatenberechnung: Python: semmap\n",
    "Benötigt sind zwei Dinge: Embeddings und Ergebnisse\n",
    "1.\tRepository clonen\n",
    "Bin -> create-embeddings (auf Kommandozeile ausführen)\n",
    "Input: Datei mit einem Typen des Vokabulars pro Zeile (cwb lexicode auf lemma-ebene oder word-ebene)\n",
    "Der Rest ist nicht nötig\n",
    "\tDas erstellt aus dem Vokabular eine Datenbank mit Embeddings für jeden Typen des Vokabulars \n",
    "\tparaphrase (Modell für englisch wählen); nur embeddings für keyword nötig\n",
    "Semspace.py -> um mit semantic space arbeiten\n",
    "Ein Semanticspace-Objekt kreieren mit input: path auf .semmap (was vorher erstellt wurde)\n",
    "Def generate2d -> items übergeben (aus der Keywordliste)\n",
    "Ergebnis: Pandas Datframe als TSV speichern\n",
    "Zwei Tabellen -> Koordinaten (x, y, 2-Dimensional) & Assoziationsmaße (z-Werte, Größe der Repräsentation)\n",
    "Visualisierung: python seaborn oder matplotlib (x, y, z)\n",
    "ajust text-> Overlap beseitigen: zu nah beieinander liegende Wörter etwas entfernen, um lesen zu können\n",
    "in R jjplot repell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut -f2 ~/trump_copy.vrt | sort | uniq > keywords.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut -f2 ~/trump_copy.vrt | sort | uniq -c | awk '$1 > 50 {print $2}' > keywords_frequent.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut -f2 ~/trump_copy.vrt \\\n",
    "  | grep -E '^[a-zA-ZäöüÄÖÜß]+$' \\\n",
    "  | sort \\\n",
    "  | uniq \\\n",
    "  > keywords_clean.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23020bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut -f2 ~/trump_copy.vrt \\\n",
    "  | grep -E '^[a-zA-ZäöüÄÖÜß]+$' \\\n",
    "  | sort \\\n",
    "  | uniq -c \\\n",
    "  | awk '$1 > 50 {print $2}' \\\n",
    "  > keywords_frequent_clean.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n semmap37 python=3.8\n",
    "conda activate semmap37\n",
    "pip install pymagnitude pandas scikit-learn sentence-transformers numpy matplotlib seaborn annoy\n",
    "git clone https://github.com/ausgerechnet/semmap.git\n",
    "cd semmap\n",
    "cd bin\n",
    "# jetzt Embeddings erstellen:\n",
    "./create-embeddings ~/keywords_frequent_clean.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df44eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create-embeddings.py ##\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from semmap.embeddings import create_embeddings\n",
    "from semmap.embeddings_store import create_embeddings_store\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"create embeddings store (three files)\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument('path_in', help=\"text file with one item per line, e.g. from 'cwb-lexdecode -P P_ATT CWB_ID'\")\n",
    "    parser.add_argument('--model_name', default='sentence-transformers/paraphrase-multilingual-mpnet-base-v2', help=\"\")\n",
    "    parser.add_argument('--path_settings', help=\"path to semmap settings file\")\n",
    "    parser.add_argument('--path_db', help=\"path to database of items\")\n",
    "    parser.add_argument('--path_annoy', help=\"path to annoy index\")\n",
    "    parser.add_argument('--n_trees', default=100, help=\"\", type=int)\n",
    "    parser.add_argument('--metric', default='angular', help=\"\")\n",
    "    parser.add_argument('--random_seed', default=42, help=\"\", type=int)\n",
    "    parser.add_argument('--as_text', action=\"store_true\", default=False, help=\"store only as gzipped text file?\")\n",
    "    parser.add_argument('--path_text_out', default=None, help=\"path to text file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.as_text:\n",
    "        path_out = f'{args.path_in}.txt.gz' if args.path_text_out is None else args.path_text_out\n",
    "        with open(args.path_in, \"rt\") as f:\n",
    "            items = f.read().rstrip().split(\"\\n\")\n",
    "        create_embeddings(items, args.model_name, path_out)\n",
    "\n",
    "    path_settings = f'{args.path_in}.semmap' if args.path_settings is None else args.path_settings\n",
    "    path_db = f'{args.path_in}.sqlite' if args.path_db is None else args.path_db\n",
    "    path_annoy = f'{args.path_in}.annoy' if args.path_annoy is None else args.path_annoy\n",
    "\n",
    "    if os.path.exists(path_settings) or os.path.exists(path_db) or os.path.exists(path_annoy):\n",
    "        raise FileExistsError()\n",
    "\n",
    "    create_embeddings_store(args.path_in, path_settings, path_db, path_annoy,\n",
    "                            args.n_trees, args.metric, args.model_name, args.random_seed)\n",
    "# args.model_name ist wohl 'default='sentence-transformers/paraphrase-multilingual-mpnet-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e62b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_embeddings in module semmap.embeddings:\n",
      "\n",
      "create_embeddings(items, model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2', path_out=None, mode='wt', as_is=False)\n",
      "    create context-free embeddings from list of types\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from semmap.embeddings import create_embeddings\n",
    "help(create_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash:\n",
    "pymagnitude:\n",
    "nano /Users/vivien/opt/anaconda3/envs/semmap311/lib/python3.11/site-packages/pymagnitude/third_party/allennlp/common/params.py\n",
    "# statt: from collections import MutableMapping, OrderedDict\n",
    "# durch das ersetzen: from collections.abc import MutableMapping\n",
    "#from collections import OrderedDict\n",
    "python -c \"from semmap.semspace import SemanticSpace; print('Import klappt jetzt komplett!')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1501ff6",
   "metadata": {},
   "source": [
    "2D erstellen (Aus Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890b1c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fertig! Die 2D-Koordinaten wurden in /Users/vivien/semmap/bin/keywords_2d.tsv gespeichert.\n",
      "              x          y\n",
      "item                      \n",
      "A    -16.102224   6.669508\n",
      "ABC    0.294166   2.549005\n",
      "AG    15.427487  -1.069275\n",
      "ASAP -29.669147  35.072575\n",
      "Abe   -5.945231  47.352264\n"
     ]
    }
   ],
   "source": [
    "from semmap.semspace import SemanticSpace\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Pfad zu deinen SemMap-Dateien\n",
    "path_base = os.path.expanduser(\"~/semmap/bin/keywords_frequent_clean.tsv\")\n",
    "\n",
    "# Prüfen, ob die Dateien existieren\n",
    "for ext in [\".semmap\", \".sqlite\", \".annoy\"]:\n",
    "    if not os.path.exists(path_base + ext):\n",
    "        raise FileNotFoundError(f\"Die Datei {path_base + ext} wurde nicht gefunden!\")\n",
    "\n",
    "# SemanticSpace-Objekt erstellen\n",
    "space = SemanticSpace(path_base + \".semmap\")\n",
    "\n",
    "# Keywords aus der TSV-Datei laden\n",
    "with open(path_base, \"r\") as f:\n",
    "    keywords = [line.strip() for line in f]\n",
    "\n",
    "# 2D-Koordinaten berechnen\n",
    "coords_df = space.generate2d(items=keywords)\n",
    "\n",
    "# Ergebnis in eine TSV-Datei speichern\n",
    "output_file = os.path.expanduser(\"~/semmap/bin/keywords_2d.tsv\")\n",
    "coords_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"✅ Fertig! Die 2D-Koordinaten wurden in {output_file} gespeichert.\")\n",
    "print(coords_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a16624",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assoziationsmaße"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# === Log-Likelihood Ratio (LLR) berechnen ===\n",
    "llr_values = {}\n",
    "\n",
    "# Gesamtzahl der Token im Korpus\n",
    "N = total_tokens\n",
    "\n",
    "for w1 in cooc_counts:\n",
    "    for w2, k11 in cooc_counts[w1].items():\n",
    "        # Häufigkeiten der einzelnen Wörter\n",
    "        f1 = token_counts.get(w1, 0)\n",
    "        f2 = token_counts.get(w2, 0)\n",
    "        \n",
    "        # 2x2 Kontingenztafel\n",
    "        k12 = f1 - k11        # w1 ohne w2\n",
    "        k21 = f2 - k11        # w2 ohne w1\n",
    "        k22 = N - (k11 + k12 + k21)\n",
    "        \n",
    "        # Erwartungswerte\n",
    "        E11 = (f1 * f2) / N\n",
    "        E12 = (f1 * (N - f2)) / N\n",
    "        E21 = ((N - f1) * f2) / N\n",
    "        E22 = ((N - f1) * (N - f2)) / N\n",
    "        \n",
    "        # Log-Likelihood berechnen\n",
    "        llr = 0\n",
    "        for k, E in [(k11, E11), (k12, E12), (k21, E21), (k22, E22)]:\n",
    "            if k > 0 and E > 0:\n",
    "                llr += k * math.log(k / E)\n",
    "        llr_values[(w1, w2)] = 2 * llr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7854f",
   "metadata": {},
   "source": [
    "Visualisierung: 2D (x, y-Werte) + Assoziationsmaße als Größe der Repräsentation (z-Werte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed571027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Pfad zu deiner 2D-Datei mit Assoziationsmaßen\n",
    "df = pd.read_csv(\"~/semmap/bin/keywords_2d.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Falls du ein z-Maß hast, sonst z.B. Frequenz oder zufällige Werte\n",
    "if 'z' not in df.columns:\n",
    "    import numpy as np\n",
    "    df['z'] = np.random.rand(len(df)) * 10  # Dummy-Werte\n",
    "\n",
    "# Interaktiver 3D-Scatter-Plot\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    z='z',\n",
    "    text='keyword',  # Name deiner Keyword-Spalte\n",
    "    color='z',       # Farben nach Z-Wert\n",
    "    size='z',        # Größe der Punkte nach Z-Wert\n",
    "    color_continuous_scale='Viridis',\n",
    "    size_max=10,\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Association Measure (Z)'\n",
    "    ),\n",
    "    title=\"Interaktive 3D-Visualisierung der Keywords\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a777146f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'adjustText'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl_toolkits\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmplot3d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01madjustText\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m adjust_text  \u001b[38;5;66;03m# pip install adjustText\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Daten laden\u001b[39;00m\n\u001b[32m      7\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m~/semmap/bin/keywords_2d.tsv\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'adjustText'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from adjustText import adjust_text  # pip install adjustText\n",
    "\n",
    "# Daten laden\n",
    "df = pd.read_csv(\"~/semmap/bin/keywords_2d.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Wenn noch kein z-Wert: z.B. Assoziationsmaß oder Dummy\n",
    "if 'z' not in df.columns:\n",
    "    import numpy as np\n",
    "    df['z'] = np.random.rand(len(df)) * 10\n",
    "\n",
    "# Figure und 3D Achse\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Punkte plotten\n",
    "sc = ax.scatter(df['x'], df['y'], df['z'], c=df['z'], cmap='viridis', s=50)\n",
    "\n",
    "# Labels vorbereiten\n",
    "texts = [ax.text(df['x'][i], df['y'][i], df['z'][i], df['keyword'][i], fontsize=9)\n",
    "         for i in range(len(df))]\n",
    "\n",
    "# Overlap bereinigen\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z (Association measure)')\n",
    "\n",
    "plt.title(\"3D-Keyword-Visualisierung mit Overlap-Anpassung\")\n",
    "plt.colorbar(sc, label='Z-Wert')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from adjustText import adjust_text  # pip install adjustText\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Daten laden\n",
    "# ------------------------------\n",
    "df = pd.read_csv(\"~/semmap/bin/keywords_2d.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Überprüfen, ob z-Wert vorhanden ist\n",
    "if 'z' not in df.columns:\n",
    "    print(\"Keine Z-Werte gefunden. Es wird ein Dummy-Wert verwendet.\")\n",
    "    df['z'] = np.random.rand(len(df)) * 10\n",
    "\n",
    "# Optional: Top-N Keywords nach z-Wert auswählen (z.B. die 50 wichtigsten)\n",
    "top_n = 50\n",
    "df_top = df.nlargest(top_n, 'z').reset_index(drop=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ 3D-Plot erstellen\n",
    "# ------------------------------\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Punkte plotten, farblich nach z-Wert\n",
    "sc = ax.scatter(df_top['x'], df_top['y'], df_top['z'], c=df_top['z'], cmap='viridis', s=60)\n",
    "\n",
    "# Labels vorbereiten\n",
    "texts = [ax.text(df_top['x'][i], df_top['y'][i], df_top['z'][i], df_top['keyword'][i], fontsize=9)\n",
    "         for i in range(len(df_top))]\n",
    "\n",
    "# Overlap bereinigen\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z (Association measure)')\n",
    "\n",
    "# Titel + Farbskala\n",
    "plt.title(f\"Top-{top_n} Keywords im 3D-SemMap-Space\", fontsize=16)\n",
    "plt.colorbar(sc, label='Z-Wert')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855f91f",
   "metadata": {},
   "source": [
    "ChatGPT komplett:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad43e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut -f2 ~/trump_copy.vrt \\\n",
    "  | grep -E '^[a-zA-ZäöüÄÖÜß]+$' \\\n",
    "  | sort \\\n",
    "  | uniq -c \\\n",
    "  | awk '{print $2 \"\\t\" $1}' \\\n",
    "  > keywords_frequent_with_counts.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc4b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ['item'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/yb/nh3fwp1s113dp86cld34qkhh0000gn/T/ipykernel_28074/1888686127.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Nur gültige Strings an SemMap übergeben\u001b[39;00m\n\u001b[32m     58\u001b[39m keywords_clean = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;28;01min\u001b[39;00m keywords_set \u001b[38;5;28;01mif\u001b[39;00m isinstance(k, str) \u001b[38;5;28;01mand\u001b[39;00m k.strip() != \u001b[33m\"\"\u001b[39m]\n\u001b[32m     59\u001b[39m coords_df = space.generate2d(items=keywords_clean)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m coords_df = coords_df.set_index(\u001b[33m\"item\"\u001b[39m)\n\u001b[32m     61\u001b[39m \n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# === Z-Wert als Summe der PMI-Werte mit anderen Tokens ===\u001b[39;00m\n\u001b[32m     63\u001b[39m z_values = {}\n",
      "\u001b[32m~/opt/anaconda3/envs/pytorch/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, keys, drop, append, inplace, verify_integrity)\u001b[39m\n\u001b[32m   6125\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m found:\n\u001b[32m   6126\u001b[39m                         missing.append(col)\n\u001b[32m   6127\u001b[39m \n\u001b[32m   6128\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m-> \u001b[39m\u001b[32m6129\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f\"None of {missing} are in the columns\")\n\u001b[32m   6130\u001b[39m \n\u001b[32m   6131\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   6132\u001b[39m             frame = self\n",
      "\u001b[31mKeyError\u001b[39m: \"None of ['item'] are in the columns\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import re\n",
    "import os\n",
    "from semmap.semspace import SemanticSpace\n",
    "import math\n",
    "\n",
    "# === Parameter ===\n",
    "corpus_file = \"trump_copy.vrt\"\n",
    "keywords_file = \"~/semmap/keywords_frequent_with_counts.tsv\"\n",
    "window_size = 5  # Wörter links/rechts für Co-Occurrence\n",
    "\n",
    "# === Keywords laden ===\n",
    "keywords_df = pd.read_csv(os.path.expanduser(keywords_file), sep=\"\\t\", header=None, names=[\"lemma\", \"freq\"])\n",
    "\n",
    "# Nur gültige Strings als Keywords behalten\n",
    "keywords_df[\"lemma\"] = keywords_df[\"lemma\"].astype(str)\n",
    "keywords_df = keywords_df[keywords_df[\"lemma\"].str.strip() != \"\"]\n",
    "keywords_set = set(keywords_df[\"lemma\"])\n",
    "\n",
    "# === Korpus lesen und Tokenisieren ===\n",
    "corpus_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) >= 2 and re.match(r'^[a-zA-ZäöüÄÖÜß]+$', parts[1]):\n",
    "            corpus_tokens.append(parts[1])\n",
    "\n",
    "# === Co-Occurrence zählen ===\n",
    "cooc_counts = defaultdict(Counter)\n",
    "for i, token in enumerate(corpus_tokens):\n",
    "    if token in keywords_set:\n",
    "        window_start = max(i - window_size, 0)\n",
    "        window_end = min(i + window_size + 1, len(corpus_tokens))\n",
    "        window_tokens = corpus_tokens[window_start:i] + corpus_tokens[i+1:window_end]\n",
    "        for t in window_tokens:\n",
    "            if t in keywords_set:\n",
    "                cooc_counts[token][t] += 1\n",
    "\n",
    "# === Assoziationsmaß berechnen (PMI) ===\n",
    "token_counts = dict(zip(keywords_df[\"lemma\"], keywords_df[\"freq\"]))\n",
    "total_tokens = sum(token_counts.values())\n",
    "\n",
    "pmi_values = {}\n",
    "for t1 in cooc_counts:\n",
    "    for t2, c in cooc_counts[t1].items():\n",
    "        p_xy = c / total_tokens\n",
    "        p_x = token_counts[t1] / total_tokens\n",
    "        p_y = token_counts[t2] / total_tokens\n",
    "        pmi_values[(t1, t2)] = math.log2(p_xy / (p_x * p_y)) if p_xy > 0 else 0\n",
    "\n",
    "# === 2D-Koordinaten aus SemMap laden ===\n",
    "path_semmap = os.path.expanduser(\"~/semmap/bin/keywords_frequent_clean.tsv.semmap\")\n",
    "space = SemanticSpace(path_semmap)\n",
    "\n",
    "# Nur gültige Strings an SemMap übergeben\n",
    "keywords_clean = [k for k in keywords_set if isinstance(k, str) and k.strip() != \"\"]\n",
    "coords_df = space.generate2d(items=keywords_clean)\n",
    "coords_df = coords_df.set_index(\"item\")\n",
    "\n",
    "# === Z-Wert als Summe der PMI-Werte mit anderen Tokens ===\n",
    "z_values = {}\n",
    "for token in keywords_clean:\n",
    "    z = sum(pmi_values.get((token, t2), 0) for t2 in keywords_clean if t2 != token)\n",
    "    z_values[token] = z\n",
    "\n",
    "# === DataFrame für 3D-Plot erstellen ===\n",
    "plot_df = coords_df.copy()\n",
    "plot_df[\"z\"] = plot_df.index.map(z_values)\n",
    "\n",
    "# === Speichern ===\n",
    "plot_df.to_csv(\"keywords_3d.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(\"3D-Daten in 'keywords_3d.tsv' gespeichert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# === Daten laden ===\n",
    "df = pd.read_csv(\"keywords_3d.tsv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "x = df['x'].values\n",
    "y = df['y'].values\n",
    "z = df['z'].values\n",
    "labels = df.index.tolist()\n",
    "\n",
    "# === 3D-Plot erstellen ===\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Punkte plotten\n",
    "sc = ax.scatter(x, y, z, c=z, cmap='viridis', s=60)\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Assoziationswert (z)')\n",
    "\n",
    "# Textlabels hinzufügen\n",
    "texts = []\n",
    "for xi, yi, zi, label in zip(x, y, z, labels):\n",
    "    texts.append(ax.text(xi, yi, zi, label, fontsize=9))\n",
    "\n",
    "# adjustText anwenden, damit sich Texte nicht überlappen\n",
    "adjust_text(texts, only_move={'points':'y', 'texts':'y'}, \n",
    "            arrowprops=dict(arrowstyle=\"-\", color='gray', lw=0.5))\n",
    "\n",
    "# Achsen\n",
    "ax.set_xlabel('SemMap x')\n",
    "ax.set_ylabel('SemMap y')\n",
    "ax.set_zlabel('Assoziationsmaß z')\n",
    "ax.set_title('3D-Visualisierung der Keywords mit Assoziationswerten')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477ff61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
