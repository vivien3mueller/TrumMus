{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae728635",
   "metadata": {},
   "source": [
    "# 01 Scraping mit Playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c680c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zur Installation der verwendeten Pakete:\n",
    "# !pip install playwright pandas aiohttp aiofiles os ssl certifi\n",
    "# einfach die Zeile mit !pip auskommentieren und durchführen; danach Kernel neu laden\n",
    "# playwright install ##in shell ausführen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbca86",
   "metadata": {},
   "source": [
    "### Wie sollen die Daten am Ende aussehen?\n",
    "- Id: Nummer des Posts\n",
    "- author: Donald Trump @realdonaldtrump\n",
    "- platform: Truthsocial or X (Twitter)\n",
    "- date: ganzes Datum (ohne Uhrzeit)\n",
    "- day: Tag des Posts\n",
    "- month: Monat des Posts\n",
    "- year: Jahr des Posts\n",
    "- time: Uhrzeit des Posts\n",
    "- text: ganzer Text (ohne Datum, Uhrzeit, Autor und Plattform)\n",
    "- image : image_path -> der Weg zu den Bildern wird lokal gespeichert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d401db5",
   "metadata": {},
   "source": [
    "### Wie sieht die Website aus:\n",
    "\n",
    "- Quellcode auf Website anschauen: https://rollcall.com/factbase-twitter/?platform=all&sort=date&sort_order=desc\n",
    "- Seite lädt Inhalte interaktiv mit Java-Script nach (nicht statisch)\n",
    "- alle Posts sind in jeweils einzelnen Blöcken gespeichert\n",
    "- im jeweiligen Block ist einmal das Bild gespeichert und zudem DAutor, Plattform, Datum, Uhrzeit und Text in einem gemeinsamen Block\n",
    "- Suchmaske auf der Website implementiert\n",
    "- tägliche Ergänzung neuer Posts\n",
    "- Blick auf die URL: Beim Scrollen ändert sich die Seitenzahl in der URL\n",
    "- zum 1.August waren es 87.640 Posts (X und Truthsocial)\n",
    "- wahrscheinlich circa 5.000 Seiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e0a8c",
   "metadata": {},
   "source": [
    "### Wahl des Tools:\n",
    "- Beautifulsoup: schon älter, nur für statische Websites geeignet, braucht länger\n",
    "- Selectolax: modern und deutlich schneller als Beautiful, wird außerdem seltener blockiert, allerdings ebenfalls nur für statische Seiten\n",
    "- Selenium: dynamische Alternative, gute Möglichkeit\n",
    "- Playwright: relativ modern, sehr schnell und effizient, für dynamische Seiten geeignet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35f94a",
   "metadata": {},
   "source": [
    "### Wie viele Posts gibt es überhaupt?\n",
    "- Auf der Webseite steht, dass es 87.656 gibt (Stand 6.8.2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78eda33",
   "metadata": {},
   "source": [
    "#### Probleme und Lösungen:\n",
    "- die Seite lädt dynamisch nach: Playwright verwenden für dynamische Webseiten\n",
    "- dynamisches Nachladen: wie komme ich zum Ende der Posts?\n",
    "- tausend doppelte und dreifache Posts: durch Seiten durchiterieren bringt leider nur doppelte Posts\n",
    "- doppelte Posts: Key erstellen, mit dem abgeglichen werden kann\n",
    "- dynamisches Nachladen der Seiten: statt durch Seiten iterieren lieber Scrollen!\n",
    "- Seiten laden langsam nach: sleep einbauen\n",
    "- durch das Herunterladen der Bilder: Programm wird sehr langsam :(, daher parallele Worker etablieren & asynchrone Methoden, statt synchron\n",
    "- Programm stürzt ab, bzw findet keine nicht immer neue Posts, weil die Seiten langsam nachladen: länger warten (sleep(3)) und Posts direkt in csv speichern und nach einem Neustart bereits Gespeichertes aus dem File laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##mit Fortsetzung bei Abbruch, final?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorhandene Datei gefunden: trump_playwright_final.csv – Lade gespeicherte Posts...\n",
      "1151 Posts bereits geladen – setze fort...\n",
      "Aktuell 1151 Posts gespeichert – 53 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 0 Posts\n",
      "Keine neuen Posts (1/5)\n",
      "Aktuell 1151 Posts gespeichert – 103 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 0 Posts\n",
      "Keine neuen Posts (2/5)\n",
      "Aktuell 1151 Posts gespeichert – 151 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 0 Posts\n",
      "Keine neuen Posts (3/5)\n",
      "Aktuell 1151 Posts gespeichert – 201 Blöcke auf der Seite sichtbar\n"
     ]
    }
   ],
   "source": [
    "# lädt langsam und ab 1000 Posts nicht mehr neu :(\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from playwright.async_api import async_playwright\n",
    "import aiohttp\n",
    "import os\n",
    "import certifi\n",
    "import ssl\n",
    "import hashlib\n",
    "import aiofiles\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "sslcontext.check_hostname = False\n",
    "sslcontext.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "CSV_FILE = \"trump_playwright_final.csv\"\n",
    "\n",
    "# Anzahl paralleler Download-Worker\n",
    "num_workers = 10\n",
    "\n",
    "async def download_worker(queue, session):\n",
    "    os.makedirs(\"images\", exist_ok=True)  # einmalig vor der Schleife\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        if item is None:  # Stop-Signal\n",
    "            queue.task_done()\n",
    "            break\n",
    "\n",
    "        image_url, post, filename = item #statt post_idx\n",
    "        try:\n",
    "            async with session.get(image_url, ssl=sslcontext) as resp:\n",
    "                if resp.status == 200:\n",
    "                    fpath = os.path.join(\"images\", filename)\n",
    "                    async with aiofiles.open(fpath, \"wb\") as f:\n",
    "                        await f.write(await resp.read())\n",
    "                    #with open(fpath, \"wb\") as f:\n",
    "                        #f.write(await resp.read())\n",
    "                    #posts_data[post_idx][\"image_path\"] = fpath\n",
    "                    post[\"image_path\"] = fpath\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Download {image_url}: {e}\")\n",
    "        finally:\n",
    "            queue.task_done()\n",
    "            \n",
    "\n",
    "            \n",
    "def extract_metadata_text(text: str):\n",
    "    lines = [line.strip() for line in text.strip().splitlines() if line.strip()]\n",
    "    author_name = \"\"\n",
    "    handle = \"\"\n",
    "    platform = \"\"\n",
    "    date_str = \"\"\n",
    "    time_str = \"\"\n",
    "    content_lines = []\n",
    "\n",
    "    if len(lines) >= 2:\n",
    "        author_name = lines[0].strip()\n",
    "        match = re.search(\n",
    "            r\"(@[\\w]+)\\s*[•\\-]\\s*(.*?)\\s*[•\\-]\\s*([A-Za-z]+ \\d{1,2}, \\d{4})\\s*@\\s*(\\d{1,2}:\\d{2} [AP]M)\",\n",
    "            lines[1]\n",
    "        )\n",
    "        if match:\n",
    "            handle = match.group(1).strip()\n",
    "            platform = match.group(2).strip()\n",
    "            date_str = match.group(3).strip()\n",
    "            time_str = match.group(4).strip()\n",
    "\n",
    "        # Content-Zeilen finden\n",
    "        #if len(lines) > 2 and lines[2].startswith(\"View\"): # statt \"View on\"\n",
    "        #    content_lines = lines[3:]\n",
    "        #else:\n",
    "        #    content_lines = lines[2:]\n",
    "        start_idx = 2\n",
    "        if len(lines) > 2 and lines[2].startswith(\"View\"):  # z.B. \"View on...\"\n",
    "            start_idx = 3\n",
    "        content_lines = lines[start_idx:]\n",
    "\n",
    "    # Text sauber zusammensetzen (mit Absatz-Trennung)\n",
    "    content_text = \"\\n\".join(content_lines).strip()\n",
    "\n",
    "    try:\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%B %d, %Y %I:%M %p\")\n",
    "        return {\n",
    "            \"author\": f\"{author_name} {handle}\".strip(),\n",
    "            \"platform\": platform,\n",
    "            \"date\": dt.strftime(\"%Y-%m-%d\"),\n",
    "            \"time\": dt.strftime(\"%H:%M\"),\n",
    "            \"year\": int(dt.year),\n",
    "            \"month\": dt.strftime(\"%B\"),\n",
    "            \"day\": int(dt.day),\n",
    "            \"text\": content_text # statt \"\\n\".join(content_lines).strip()\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"author\": f\"{author_name} {handle}\".strip(),\n",
    "            \"platform\": platform,\n",
    "            \"date\": date_str,\n",
    "            \"time\": time_str,\n",
    "            \"year\": \"\",\n",
    "            \"month\": \"\",\n",
    "            \"day\": \"\",\n",
    "            \"text\": content_text #\"\\n\".join(content_lines).strip()\n",
    "        }\n",
    "# -------- Duplikat-Key --------\n",
    "def make_post_key(data, include_image=True, include_text=True):\n",
    "    \"\"\"\n",
    "    Generiert einen eindeutigen Schlüssel für einen Post.\n",
    "    - Text optional\n",
    "    - Bild optional\n",
    "    - Nur Posts ohne jegliche Metadaten UND ohne Text UND ohne Bild werden verworfen\n",
    "    \"\"\"\n",
    "\n",
    "    # Metadaten absichern (NaN -> \"\", None -> \"\")\n",
    "    author = str(data.get(\"author\", \"\") or \"\").strip()\n",
    "    platform = str(data.get(\"platform\", \"\") or \"\").strip()\n",
    "    date = str(data.get(\"date\", \"\") or \"\").strip()\n",
    "    time = str(data.get(\"time\", \"\") or \"\").strip()\n",
    "    text = str(data.get(\"text\", \"\") or \"\").strip()\n",
    "    img = data.get(\"image_path\")\n",
    "\n",
    "    # Wenn absolut nichts da → kein valider Key\n",
    "    if not (author or platform or date or text or img):\n",
    "        return None\n",
    "\n",
    "    parts = []\n",
    "    parts.extend([author, platform, date, time])\n",
    "\n",
    "    # Text einbauen (optional)\n",
    "    if include_text and text:\n",
    "        # normalize whitespace → verhindert Unterschiede nur wegen \\n oder Mehrfach-Leerzeichen\n",
    "        text_norm = re.sub(r\"\\s+\", \" \", text)\n",
    "        parts.append(text_norm)\n",
    "\n",
    "    # Bild einbauen (optional)\n",
    "    if include_image and isinstance(img, str) and img.strip():\n",
    "        parts.append(os.path.basename(img.strip()))\n",
    "\n",
    "    # Key zusammenbauen\n",
    "    raw_key = \"|\".join(parts).strip()\n",
    "    if not raw_key:\n",
    "        return None\n",
    "\n",
    "    return hashlib.md5(raw_key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "async def scrape_all_dynamic(max_posts=900, max_no_new=5):\n",
    "    posts_data = []\n",
    "    seen_posts = set()\n",
    "\n",
    "    # --- Fortsetzung ---\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        print(f\"Vorhandene Datei gefunden: {CSV_FILE} – Lade gespeicherte Posts...\")\n",
    "        df_existing = pd.read_csv(CSV_FILE).fillna(\"\") #fillna neu\n",
    "        posts_data = df_existing.to_dict(\"records\")\n",
    "        seen_posts = {make_post_key(row) for row in posts_data if make_post_key(row)} #ab if neu\n",
    "        print(f\"{len(posts_data)} Posts bereits geladen – setze fort...\")\n",
    "    else:\n",
    "        posts_data = []\n",
    "        seen_posts = set()\n",
    "\n",
    "    no_new_rounds = 0\n",
    "\n",
    "    async with async_playwright() as p, aiohttp.ClientSession() as session:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        page.set_default_timeout(60000)\n",
    "\n",
    "        await page.goto(\"https://rollcall.com/factbase-twitter/?platform=all&sort=date&sort_order=desc\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        queue = asyncio.Queue()\n",
    "        workers = [asyncio.create_task(download_worker(queue, session)) for _ in range(num_workers)]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                await page.wait_for_selector(\"div.block\", timeout=30000)\n",
    "                blocks = await page.query_selector_all(\"div.block\")\n",
    "            except:\n",
    "                print(\"Keine weiteren Posts, breche ab.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Aktuell {len(posts_data)} Posts gespeichert – {len(blocks)} Blöcke auf der Seite sichtbar\")\n",
    "\n",
    "            new_count = 0\n",
    "            for block in blocks:\n",
    "                if len(posts_data) >= max_posts:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    full_text = await block.inner_text()\n",
    "                    data = extract_metadata_text(full_text)\n",
    "\n",
    "                    # Basis-Post\n",
    "                    post = {\n",
    "                        \"author\": data[\"author\"],\n",
    "                        \"platform\": data[\"platform\"],\n",
    "                        \"date\": data[\"date\"],\n",
    "                        \"time\": data[\"time\"],\n",
    "                        \"day\": data[\"day\"],\n",
    "                        \"month\": data[\"month\"],\n",
    "                        \"year\": data[\"year\"],\n",
    "                        \"text\": data[\"text\"],\n",
    "                        \"image_path\": None,\n",
    "                        \"image_url\": None #neu\n",
    "                    }\n",
    "\n",
    "                    # Bild-Download vorbereiten\n",
    "                    img_src = None #neu\n",
    "                    try:\n",
    "                        img_el = await block.query_selector(\"img\")\n",
    "                        if img_el:\n",
    "                            src = await img_el.get_attribute(\"src\")\n",
    "                            if src and re.search(r\"\\.jpe?g\", src, re.IGNORECASE):\n",
    "                                post[\"image_url\"] = src #neu\n",
    "                                img_src = src #neu\n",
    "                                filename = f\"{hashlib.md5(src.encode()).hexdigest()}.jpg\"\n",
    "                                post[\"image_path\"] = filename #neu\n",
    "                                await queue.put((src, post, filename))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Bildfehler: {e}\")\n",
    "\n",
    "                    post[\"image_url\"] = img_src #neu\n",
    "                    \n",
    "                    # Schlüssel generieren\n",
    "                    key = make_post_key(post, include_text=True, include_image=True)\n",
    "                    if not key or key in seen_posts:\n",
    "                        continue\n",
    "                    seen_posts.add(key)\n",
    "                    \n",
    "                    posts_data.append(post)\n",
    "                    new_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei Post: {e}\")\n",
    "\n",
    "            print(f\"Neu hinzugekommen: {new_count} Posts\")\n",
    "\n",
    "            # Scrollen und no_new_rounds prüfen\n",
    "            if new_count == 0:\n",
    "                no_new_rounds += 1\n",
    "                print(f\"Keine neuen Posts ({no_new_rounds}/{max_no_new})\")\n",
    "                if no_new_rounds >= max_no_new:\n",
    "                    break\n",
    "            else:\n",
    "                no_new_rounds = 0\n",
    "\n",
    "            # Scrollen nur, wenn max_posts noch nicht erreicht\n",
    "            if len(posts_data) < max_posts:\n",
    "                last_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "                await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "                await asyncio.sleep(3)\n",
    "                new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    print(f\"Scrollen brachte nichts Neues ({no_new_rounds}/{max_no_new})\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Maximale Anzahl {max_posts} erreicht.\")\n",
    "                break\n",
    "\n",
    "        # Queue abwarten\n",
    "        await queue.join()\n",
    "        for _ in range(num_workers):\n",
    "            await queue.put(None)\n",
    "        await asyncio.gather(*workers)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # IDs vergeben und CSV speichern\n",
    "    for idx, post in enumerate(posts_data, start=1):\n",
    "        post[\"id\"] = idx\n",
    "\n",
    "    fff = pd.DataFrame(posts_data)\n",
    "    cols = [\"id\", \"author\", \"platform\", \"date\", \"time\", \"day\", \"month\", \"year\", \"text\", \"image_path\"]\n",
    "    fff = fff[cols]\n",
    "    fff.to_csv(CSV_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Scraping abgeschlossen. Gesamt: {len(fff)} Posts.\")\n",
    "\n",
    "\n",
    "# --- Analyse: Wie viele Posts enthalten Text ---\n",
    "    total = len(fff)\n",
    "    no_text = fff[\"text\"].isna().sum()\n",
    "    with_text = total - no_text\n",
    "    print(\"===================================\")\n",
    "    print(f\"Gesamt:     {total}\")\n",
    "    print(f\"Ohne Text:  {no_text} ({no_text/total:.1%})\")\n",
    "    print(f\"Mit Text:   {with_text} ({with_text/total:.1%})\")\n",
    "    print(\"===================================\")\n",
    "\n",
    "# Starten\n",
    "await scrape_all_dynamic(max_posts=90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36895b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorhandene Datei gefunden: factbase_posts_clean.csv – Lade gespeicherte Posts...\n",
      "6381 Posts bereits geladen – setze fort...\n",
      "Aktuell 6381 Posts gespeichert – 55 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 5 Posts\n",
      "Aktuell 6386 Posts gespeichert – 105 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 4 Posts\n",
      "Aktuell 6390 Posts gespeichert – 151 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 8 Posts\n",
      "Aktuell 6398 Posts gespeichert – 201 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 9 Posts\n",
      "Aktuell 6407 Posts gespeichert – 251 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 6 Posts\n",
      "Aktuell 6413 Posts gespeichert – 301 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 7 Posts\n",
      "Aktuell 6420 Posts gespeichert – 351 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 8 Posts\n",
      "Aktuell 6428 Posts gespeichert – 401 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 4 Posts\n",
      "Aktuell 6432 Posts gespeichert – 451 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 5 Posts\n",
      "Aktuell 6437 Posts gespeichert – 501 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 10 Posts\n",
      "Aktuell 6447 Posts gespeichert – 551 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 4 Posts\n",
      "Aktuell 6451 Posts gespeichert – 601 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 9 Posts\n",
      "Aktuell 6460 Posts gespeichert – 651 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 11 Posts\n",
      "Aktuell 6471 Posts gespeichert – 701 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 8 Posts\n",
      "Aktuell 6479 Posts gespeichert – 751 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 10 Posts\n",
      "Aktuell 6489 Posts gespeichert – 801 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 8 Posts\n",
      "Aktuell 6497 Posts gespeichert – 851 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 11 Posts\n",
      "Aktuell 6508 Posts gespeichert – 901 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 11 Posts\n",
      "Aktuell 6519 Posts gespeichert – 951 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 18 Posts\n",
      "Aktuell 6537 Posts gespeichert – 1001 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 18 Posts\n",
      "Aktuell 6555 Posts gespeichert – 1051 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 8 Posts\n",
      "Aktuell 6563 Posts gespeichert – 1101 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 7 Posts\n",
      "Aktuell 6570 Posts gespeichert – 1151 Blöcke auf der Seite sichtbar\n",
      "Neu hinzugekommen: 6 Posts\n",
      "Aktuell 6576 Posts gespeichert – 1201 Blöcke auf der Seite sichtbar\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from playwright.async_api import async_playwright\n",
    "import aiohttp\n",
    "import os\n",
    "import certifi\n",
    "import ssl\n",
    "import hashlib\n",
    "import aiofiles\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "sslcontext.check_hostname = False\n",
    "sslcontext.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "CSV_FILE = \"factbase_posts_clean.csv\" #\"trump_playwright_final.csv\"\n",
    "\n",
    "# Anzahl paralleler Download-Worker\n",
    "num_workers = 10\n",
    "\n",
    "async def download_worker(queue, session):\n",
    "    os.makedirs(\"images\", exist_ok=True)  # einmalig vor der Schleife\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        if item is None:  # Stop-Signal\n",
    "            queue.task_done()\n",
    "            break\n",
    "\n",
    "        image_url, post, filename = item #statt post_idx\n",
    "        try:\n",
    "            async with session.get(image_url, ssl=sslcontext) as resp:\n",
    "                if resp.status == 200:\n",
    "                    fpath = os.path.join(\"images\", filename)\n",
    "                    async with aiofiles.open(fpath, \"wb\") as f:\n",
    "                        await f.write(await resp.read())\n",
    "                    post[\"image_path\"] = fpath\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Download {image_url}: {e}\")\n",
    "        finally:\n",
    "            queue.task_done()\n",
    "            \n",
    "\n",
    "            \n",
    "def extract_metadata_text(text: str):\n",
    "    lines = [line.strip() for line in text.strip().splitlines() if line.strip()]\n",
    "    author_name = \"\"\n",
    "    handle = \"\"\n",
    "    platform = \"\"\n",
    "    date_str = \"\"\n",
    "    time_str = \"\"\n",
    "    content_lines = []\n",
    "\n",
    "    if len(lines) >= 2:\n",
    "        author_name = lines[0].strip()\n",
    "        match = re.search(\n",
    "            r\"(@[\\w]+)\\s*[•\\-]\\s*(.*?)\\s*[•\\-]\\s*([A-Za-z]+ \\d{1,2}, \\d{4})\\s*@\\s*(\\d{1,2}:\\d{2} [AP]M)\",\n",
    "            lines[1]\n",
    "        )\n",
    "        if match:\n",
    "            handle = match.group(1).strip()\n",
    "            platform = match.group(2).strip()\n",
    "            date_str = match.group(3).strip()\n",
    "            time_str = match.group(4).strip()\n",
    "\n",
    "        start_idx = 2\n",
    "        if len(lines) > 2 and lines[2].startswith(\"View\"):  # z.B. \"View on ...\"\n",
    "            start_idx = 3\n",
    "        content_lines = lines[start_idx:]\n",
    "\n",
    "    # Absatzmarker durch Leerzeichen ersetzen\n",
    "    content_text = \" \".join(content_lines).strip()\n",
    "\n",
    "    # Doppelte Leerzeichen normalisieren\n",
    "    content_text = re.sub(r\"\\s{2,}\", \" \", content_text)\n",
    "\n",
    "    try:\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%B %d, %Y %I:%M %p\")\n",
    "        return {\n",
    "            \"author\": f\"{author_name} {handle}\".strip(),\n",
    "            \"platform\": platform,\n",
    "            \"date\": dt.strftime(\"%Y-%m-%d\"),\n",
    "            \"time\": dt.strftime(\"%H:%M\"),\n",
    "            \"year\": int(dt.year),\n",
    "            \"month\": dt.strftime(\"%B\"),\n",
    "            \"day\": int(dt.day),\n",
    "            \"text\": content_text\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"author\": f\"{author_name} {handle}\".strip(),\n",
    "            \"platform\": platform,\n",
    "            \"date\": date_str,\n",
    "            \"time\": time_str,\n",
    "            \"year\": \"\",\n",
    "            \"month\": \"\",\n",
    "            \"day\": \"\",\n",
    "            \"text\": content_text\n",
    "        }\n",
    "    \n",
    "# -------- Duplikat-Key --------\n",
    "def make_post_key(data, include_image=False, include_text=True):\n",
    "    \"\"\"\n",
    "    Generiert einen eindeutigen Schlüssel für einen Post.\n",
    "    - Text optional\n",
    "    - Bild optional\n",
    "    - Nur Posts ohne jegliche Metadaten UND ohne Text UND ohne Bild werden verworfen\n",
    "    \"\"\"\n",
    "    # --- Gültigkeit prüfen ---\n",
    "    # -> wenn keine Metadaten vorhanden, kein Key\n",
    "    # Metadaten prüfen\n",
    "    #author = str(data.get(\"author\", \"\")).strip()\n",
    "    platform = str(data.get(\"platform\", \"\")).strip()\n",
    "    date = str(data.get(\"date\", \"\")).strip()\n",
    "    time = str(data.get(\"time\", \"\")).strip()\n",
    "    #text = str(data.get(\"text\", \"\")).strip()\n",
    "    img = data.get(\"image_path\")\n",
    "    \n",
    "     # Wenn gar nichts da ist → kein valider Key\n",
    "    if not (platform or date or time or img): #Text kann auch leer sein, daher weglassen\n",
    "        return None\n",
    "    \n",
    "    parts = []\n",
    "    # Basis-Metadaten immer einbauen\n",
    "    parts.extend([platform, date, time])\n",
    "\n",
    "    # Text (optional)\n",
    "    if include_text:\n",
    "        text_val = str(data.get(\"text\", \"\")).strip().lower()\n",
    "        text_norm = re.sub(r\"\\s+\", \" \", text_val)\n",
    "        parts.append(text_norm)\n",
    "    \n",
    "    # Bild (optional)\n",
    "    if include_image and isinstance(img, str) and img.strip():\n",
    "        parts.append(os.path.basename(img.strip()))\n",
    "\n",
    "    raw_key = \"|\".join(parts).strip()\n",
    "    if not raw_key:\n",
    "        return None\n",
    "\n",
    "    return hashlib.md5(raw_key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "async def scrape_all_dynamic(max_posts=90000, max_no_new=5):\n",
    "    posts_data = []\n",
    "    seen_posts = set()\n",
    "\n",
    "    # --- Fortsetzung ---\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        print(f\"Vorhandene Datei gefunden: {CSV_FILE} – Lade gespeicherte Posts...\")\n",
    "        df_existing = pd.read_csv(CSV_FILE)\n",
    "        posts_data = df_existing.to_dict(\"records\")\n",
    "        seen_posts = {make_post_key(row) for row in posts_data}\n",
    "        print(f\"{len(posts_data)} Posts bereits geladen – setze fort...\")\n",
    "\n",
    "    no_new_rounds = 0\n",
    "\n",
    "    async with async_playwright() as p, aiohttp.ClientSession() as session:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        page.set_default_timeout(60000)\n",
    "\n",
    "        await page.goto(\"https://rollcall.com/factbase-twitter/?platform=all&sort=date&sort_order=desc\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        queue = asyncio.Queue()\n",
    "        workers = [asyncio.create_task(download_worker(queue, session)) for _ in range(num_workers)]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                await page.wait_for_selector(\"div.block\", timeout=30000)\n",
    "                blocks = await page.query_selector_all(\"div.block\")\n",
    "            except:\n",
    "                print(\"Keine weiteren Posts, breche ab.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Aktuell {len(posts_data)} Posts gespeichert – {len(blocks)} Blöcke auf der Seite sichtbar\")\n",
    "\n",
    "            new_count = 0\n",
    "            for block in blocks:\n",
    "                if len(posts_data) >= max_posts:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    full_text = await block.inner_text()\n",
    "                    data = extract_metadata_text(full_text)\n",
    "\n",
    "                    # Basis-Post\n",
    "                    post = {\n",
    "                        \"author\": data[\"author\"],\n",
    "                        \"platform\": data[\"platform\"],\n",
    "                        \"date\": data[\"date\"],\n",
    "                        \"time\": data[\"time\"],\n",
    "                        \"day\": data[\"day\"],\n",
    "                        \"month\": data[\"month\"],\n",
    "                        \"year\": data[\"year\"],\n",
    "                        \"text\": data[\"text\"],\n",
    "                        \"image_path\": None,\n",
    "                        \"image_url\": None #neu\n",
    "                    }\n",
    "\n",
    "                    # Bild-Download vorbereiten\n",
    "                    img_src = None #neu\n",
    "                    try:\n",
    "                        img_el = await block.query_selector(\"img\")\n",
    "                        if img_el:\n",
    "                            src = await img_el.get_attribute(\"src\")\n",
    "                            if src and re.search(r\"\\.jpe?g\", src, re.IGNORECASE):\n",
    "                                img_src = src #neu\n",
    "                                filename = f\"{hashlib.md5(src.encode()).hexdigest()}.jpg\"\n",
    "                                await queue.put((src, post, filename))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    post[\"image_url\"] = img_src #neu\n",
    "                    \n",
    "                    # Schlüssel generieren\n",
    "                    key = make_post_key(post, include_image=False, include_text=True)\n",
    "                    if not key or key in seen_posts:\n",
    "                        continue\n",
    "                    seen_posts.add(key)\n",
    "                    \n",
    "                    posts_data.append(post)\n",
    "                    new_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei Post: {e}\")\n",
    "\n",
    "            print(f\"Neu hinzugekommen: {new_count} Posts\")\n",
    "\n",
    "            # Scrollen und no_new_rounds prüfen\n",
    "            if new_count == 0:\n",
    "                no_new_rounds += 1\n",
    "                print(f\"Keine neuen Posts ({no_new_rounds}/{max_no_new})\")\n",
    "                if no_new_rounds >= max_no_new:\n",
    "                    break\n",
    "            else:\n",
    "                no_new_rounds = 0\n",
    "\n",
    "            # Scrollen nur, wenn max_posts noch nicht erreicht\n",
    "            if len(posts_data) < max_posts:\n",
    "                last_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "                await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "                await asyncio.sleep(3)\n",
    "                new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    print(f\"Scrollen brachte nichts Neues ({no_new_rounds}/{max_no_new})\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Maximale Anzahl {max_posts} erreicht.\")\n",
    "                break\n",
    "\n",
    "        # Queue abwarten\n",
    "        await queue.join()\n",
    "        for _ in range(num_workers):\n",
    "            await queue.put(None)\n",
    "        await asyncio.gather(*workers)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # IDs vergeben und CSV speichern\n",
    "    for idx, post in enumerate(posts_data, start=1):\n",
    "        post[\"id\"] = idx\n",
    "\n",
    "    fff = pd.DataFrame(posts_data)\n",
    "    cols = [\"id\", \"author\", \"platform\", \"date\", \"time\", \"day\", \"month\", \"year\", \"text\", \"image_path\"]\n",
    "    fff = fff[cols]\n",
    "    fff.to_csv(CSV_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Scraping abgeschlossen. Gesamt: {len(fff)} Posts.\")\n",
    "\n",
    "\n",
    "# --- Analyse: Wie viele Posts enthalten Text ---\n",
    "    total = len(fff)\n",
    "    no_text = fff[\"text\"].isna().sum()\n",
    "    with_text = total - no_text\n",
    "    print(\"===================================\")\n",
    "    print(f\"Gesamt:     {total}\")\n",
    "    print(f\"Ohne Text:  {no_text} ({no_text/total:.1%})\")\n",
    "    print(f\"Mit Text:   {with_text} ({with_text/total:.1%})\")\n",
    "    print(\"===================================\")\n",
    "\n",
    "# Starten\n",
    "await scrape_all_dynamic(max_posts=90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2df45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         5846  \\\n",
      "id                                                       5847   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                         Truth Social   \n",
      "date                                               2024-11-03   \n",
      "time                                                    01:25   \n",
      "day                                                       3.0   \n",
      "month                                                November   \n",
      "year                                                   2024.0   \n",
      "text        If Kamala wins, you are 3 days away from the s...   \n",
      "image_path        images/c686ddbd8c368873f7932d52eb10d5e1.jpg   \n",
      "\n",
      "                                                         5847  \\\n",
      "id                                                       5848   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                                  NaN   \n",
      "date                                               2024-11-03   \n",
      "time                                                    01:25   \n",
      "day                                                       3.0   \n",
      "month                                                November   \n",
      "year                                                   2024.0   \n",
      "text        As we rescue our economy, I will also restore ...   \n",
      "image_path        images/a766c74e93656b1603a77f76a7f8bcc9.jpg   \n",
      "\n",
      "                                                         5848  \\\n",
      "id                                                       5849   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                                  NaN   \n",
      "date                                               2024-11-03   \n",
      "time                                                    01:23   \n",
      "day                                                       3.0   \n",
      "month                                                November   \n",
      "year                                                   2024.0   \n",
      "text        If Kamala wins, you are 3 days away from the s...   \n",
      "image_path        images/0eac325c150f7d6486be36d5b773d698.jpg   \n",
      "\n",
      "                                                         5849  \\\n",
      "id                                                       5850   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                         Truth Social   \n",
      "date                                               2024-11-03   \n",
      "time                                                    01:20   \n",
      "day                                                       3.0   \n",
      "month                                                November   \n",
      "year                                                   2024.0   \n",
      "text        https://www.breitbart.com/the-media/2024/11/02...   \n",
      "image_path        images/a94100a8d3129c0a94b9c91d34d45f8f.jpg   \n",
      "\n",
      "                                                         5850  \\\n",
      "id                                                       5851   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                         Truth Social   \n",
      "date                                               2024-11-03   \n",
      "time                                                    01:13   \n",
      "day                                                       3.0   \n",
      "month                                                November   \n",
      "year                                                   2024.0   \n",
      "text        https://www.breitbart.com/clips/2009/10/05/pis...   \n",
      "image_path        images/68f0a1c06ad6f5ebaad7fbd9c5789eec.jpg   \n",
      "\n",
      "                                                   5851  \\\n",
      "id                                                 5852   \n",
      "author                    Donald Trump @realDonaldTrump   \n",
      "platform                                            NaN   \n",
      "date                                         2024-11-03   \n",
      "time                                              00:18   \n",
      "day                                                 3.0   \n",
      "month                                          November   \n",
      "year                                             2024.0   \n",
      "text                            https://t.co/xE7d8Uo6qz   \n",
      "image_path  images/4454409a62996ba57a8326fc8510d852.jpg   \n",
      "\n",
      "                                                   5852  \\\n",
      "id                                                 5853   \n",
      "author                    Donald Trump @realDonaldTrump   \n",
      "platform                                   Truth Social   \n",
      "date                                         2024-11-02   \n",
      "time                                              23:57   \n",
      "day                                                 2.0   \n",
      "month                                          November   \n",
      "year                                             2024.0   \n",
      "text                                                NaN   \n",
      "image_path  images/39683b4916fca04a5a6f2ee720bacf73.jpg   \n",
      "\n",
      "                                                         5853  \\\n",
      "id                                                       5854   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                                  NaN   \n",
      "date                                               2024-11-02   \n",
      "time                                                    23:37   \n",
      "day                                                       2.0   \n",
      "month                                                November   \n",
      "year                                                   2024.0   \n",
      "text        Three beautiful MAGA RALLIES today in Gastonia...   \n",
      "image_path        images/91ced3b8614064c8ffc5620e59aa03bf.jpg   \n",
      "\n",
      "                                                         5854  \\\n",
      "id                                                       5855   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                                  NaN   \n",
      "date                                               2024-11-02   \n",
      "time                                                    23:29   \n",
      "day                                                       2.0   \n",
      "month                                                November   \n",
      "year                                                   2024.0   \n",
      "text        THANK YOU—GREENSBORO, NORTH CAROLINA! #MAGA ht...   \n",
      "image_path        images/00a8daa2f3c80102ee965af26597442d.jpg   \n",
      "\n",
      "                                                    5855  \n",
      "id                                                  5856  \n",
      "author                     Donald Trump @realDonaldTrump  \n",
      "platform                                    Truth Social  \n",
      "date                                          2024-11-02  \n",
      "time                                               23:13  \n",
      "day                                                  2.0  \n",
      "month                                           November  \n",
      "year                                              2024.0  \n",
      "text        RT @realDonaldTrump11/2/24 | SALEM, VIRGINIA  \n",
      "image_path   images/a7f7eba9fe3e8c413f3ec4678e5ed305.jpg  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ppp = pd.read_csv(\"factbase_posts_clean.csv\")\n",
    "print(ppp.tail(10).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deef621c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5856, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886e23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  26775  \\\n",
      "id                                                26776   \n",
      "author                    Donald Trump @realDonaldTrump   \n",
      "platform                                   Truth Social   \n",
      "date                                         2025-02-23   \n",
      "time                                              12:20   \n",
      "day                                                23.0   \n",
      "month                                          February   \n",
      "year                                             2025.0   \n",
      "text                                                NaN   \n",
      "image_path  images/f069d362f328857b3dfc3009e9d2e370.jpg   \n",
      "\n",
      "                                                  26776  \\\n",
      "id                                                26777   \n",
      "author                    Donald Trump @realDonaldTrump   \n",
      "platform                                   Truth Social   \n",
      "date                                         2025-02-22   \n",
      "time                                              22:41   \n",
      "day                                                22.0   \n",
      "month                                          February   \n",
      "year                                             2025.0   \n",
      "text                                                NaN   \n",
      "image_path  images/e3e6cf7eb26f784d0e10e73b659d5316.jpg   \n",
      "\n",
      "                                                  26777  \\\n",
      "id                                                26778   \n",
      "author                    Donald Trump @realDonaldTrump   \n",
      "platform                                   Truth Social   \n",
      "date                                         2025-02-22   \n",
      "time                                              22:39   \n",
      "day                                                22.0   \n",
      "month                                          February   \n",
      "year                                             2025.0   \n",
      "text                                                NaN   \n",
      "image_path  images/2ae47290e5a14c68e68d08e85844ba08.jpg   \n",
      "\n",
      "                                                        26778  \\\n",
      "id                                                      26779   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                         Truth Social   \n",
      "date                                               2025-02-22   \n",
      "time                                                    18:22   \n",
      "day                                                      22.0   \n",
      "month                                                February   \n",
      "year                                                   2025.0   \n",
      "text        Melania and I are looking forward to dinner to...   \n",
      "image_path        images/55fddad6395365edd40b0bc8aba38af0.jpg   \n",
      "\n",
      "                                                        26779  \\\n",
      "id                                                      26780   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                         Truth Social   \n",
      "date                                               2025-02-22   \n",
      "time                                                    18:12   \n",
      "day                                                      22.0   \n",
      "month                                                February   \n",
      "year                                                   2025.0   \n",
      "text        Remember, LARA TRUMP opens her big new show to...   \n",
      "image_path        images/f86e3037b2e8c3c83af542cf2ebd747e.jpg   \n",
      "\n",
      "                                                        26780  \\\n",
      "id                                                      26781   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                         Truth Social   \n",
      "date                                               2025-02-22   \n",
      "time                                                    14:03   \n",
      "day                                                      22.0   \n",
      "month                                                February   \n",
      "year                                                   2025.0   \n",
      "text        https://rumble.com/v6lwpvv-live-cpac-2025-day-...   \n",
      "image_path        images/d7988e3b896699480be6f76615b8e6d5.jpg   \n",
      "\n",
      "                                                     26781  \\\n",
      "id                                                   26782   \n",
      "author                       Donald Trump @realDonaldTrump   \n",
      "platform                                      Truth Social   \n",
      "date                                            2025-02-22   \n",
      "time                                                 08:19   \n",
      "day                                                   22.0   \n",
      "month                                             February   \n",
      "year                                                2025.0   \n",
      "text        SIXTH HOSTAGE HAS JUST BEEN RELEASED BY HAMAS!   \n",
      "image_path     images/72b6ea0d1066995893de888334baa56e.jpg   \n",
      "\n",
      "                                                  26782  \\\n",
      "id                                                26783   \n",
      "author                    Donald Trump @realDonaldTrump   \n",
      "platform                                   Truth Social   \n",
      "date                                         2025-02-22   \n",
      "time                                              08:14   \n",
      "day                                                22.0   \n",
      "month                                          February   \n",
      "year                                             2025.0   \n",
      "text                   OUR SOUTHERN BORDER IS CLOSED!!!   \n",
      "image_path  images/b029b1b1dc3fea735f90f4b1349390cb.jpg   \n",
      "\n",
      "                                                        26783  \\\n",
      "id                                                      26784   \n",
      "author                          Donald Trump @realDonaldTrump   \n",
      "platform                                         Truth Social   \n",
      "date                                               2025-02-22   \n",
      "time                                                    08:12   \n",
      "day                                                      22.0   \n",
      "month                                                February   \n",
      "year                                                   2025.0   \n",
      "text        MSNBC, COMMONLY KNOWN AS MSDNC, IS A THREAT TO...   \n",
      "image_path        images/31722dc50d0daae767ea39448598cb32.jpg   \n",
      "\n",
      "                                                        26784  \n",
      "id                                                      26785  \n",
      "author                          Donald Trump @realDonaldTrump  \n",
      "platform                                         Truth Social  \n",
      "date                                               2025-02-22  \n",
      "time                                                    08:04  \n",
      "day                                                      22.0  \n",
      "month                                                February  \n",
      "year                                                   2025.0  \n",
      "text        ELON IS DOING A GREAT JOB, BUT I WOULD LIKE TO...  \n",
      "image_path        images/50e4f4d09f35ac1997a6014329c86ba4.jpg  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fff = pd.read_csv(\"trump_playwright_final.csv\")\n",
    "print(fff.tail(10).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85df2c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hannity.com/media-room/out-here-in-these-streets-trump-to-patrol-d-c-with-police-military-as-part-of-crime-crackdown-report/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b286e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26785, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "434fdee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 NaN-Werte\n",
      "0 leere Strings\n"
     ]
    }
   ],
   "source": [
    "# Wie viele leere Werten gibt es?\n",
    "print(fff['time'].isna().sum(), \"NaN-Werte\")\n",
    "print((fff['time'] == \"\").sum(), \"leere Strings\")\n",
    "#print(fff['time'].apply(lambda x: repr(str(x))).value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63f84552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Posts ohne Zeitangabe: 1\n"
     ]
    }
   ],
   "source": [
    "# Wie viele Posts enthalten keine Angabe zur Zeit?\n",
    "count_without_time = fff[\"time\"].isna().sum()\n",
    "print(f\"Anzahl der Posts ohne Zeitangabe: {count_without_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "483b698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorher: 5928 Zeilen, danach: 5928 Zeilen ohne Duplikate.\n"
     ]
    }
   ],
   "source": [
    "fff_single = fff.drop_duplicates(subset=['time','date', 'image_path'], keep=\"first\")\n",
    "print(f\"Vorher: {len(fff)} Zeilen, danach: {len(fff_single)} Zeilen ohne Duplikate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ece5e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorher: 5856 Zeilen, danach: 5179 Zeilen ohne Duplikate.\n"
     ]
    }
   ],
   "source": [
    "ppp_single = ppp.drop_duplicates(subset=['time','date', 'image_path'], keep=\"first\")\n",
    "print(f\"Vorher: {len(ppp)} Zeilen, danach: {len(ppp_single)} Zeilen ohne Duplikate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eef13a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Posts ohne Text: 0\n"
     ]
    }
   ],
   "source": [
    "count_without_text = fff[\"id\"].isna().sum()\n",
    "print(f\"Anzahl der Posts ohne Text: {count_without_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfa7932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereinigung der Nan-Werte\n",
    "fff['text'] = fff['text'].fillna(\"\").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3fc11f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, author, platform, date, time, day, month, year, text, image_path]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "post_452 = fff[fff['time'] == '04:52']\n",
    "print(post_452)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb885d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Übersicht ==\n",
      "Gleicher Text (ALLE, inkl. exakter):  26059\n",
      "Gleiche Bilder (ALLE, inkl. exakter): 25935\n",
      "Exakte Duplikat-Zeilen:               25935\n",
      "Exakte Duplikate mit leerem Text: 4591\n",
      "Exakte Duplikate mit echtem Text: 21344\n",
      "Nur Text-Duplikate (nicht exakt):     124\n",
      "Nur Bild-Duplikate (nicht exakt):     0\n",
      "Text+Bild dupl. (nicht exakt):        0\n",
      "Echte einzigartige Zeilen:            726\n",
      "Gesamt:                               26785\n",
      "Gespeichert: groups_text_all.csv (26059 Zeilen)\n",
      "Gespeichert: groups_text_only.csv (124 Zeilen)\n",
      "Gespeichert: groups_exact.csv (25935 Zeilen)\n",
      "Gespeichert: groups_images_all.csv (25935 Zeilen)\n",
      "Keine Gruppen für groups_images_only.csv\n",
      "Keine Gruppen für groups_mixed.csv\n",
      "Gespeichert: unique_posts.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Normalisierung wie besprochen ---\n",
    "def normalize_text(x):\n",
    "    if pd.isna(x):\n",
    "        return \"<EMPTY>\"\n",
    "    txt = str(x).strip().lower()\n",
    "    if txt == \"\":\n",
    "        return \"<EMPTY>\"\n",
    "    # Links dürfen bleiben\n",
    "    return txt\n",
    "\n",
    "fff[\"text_norm\"]   = fff[\"text\"].apply(normalize_text)\n",
    "fff[\"author_norm\"] = fff[\"author\"].fillna(\"\").str.strip().str.lower()\n",
    "fff[\"img_norm\"]    = fff[\"image_path\"].fillna(\"\").str.strip().str.lower()\n",
    "fff.loc[fff[\"img_norm\"]==\"\", \"img_norm\"] = \"<NO_IMAGE>\"\n",
    "\n",
    "mask_empty_text = fff[\"text_norm\"] == \"<EMPTY>\"\n",
    "mask_no_image   = fff[\"img_norm\"]  == \"<NO_IMAGE>\"\n",
    "\n",
    "# --- Grundmengen (Zeilenebene) ---\n",
    "exact_rows       = fff.duplicated([\"author_norm\",\"platform\",\"date\",\"time\",\"text_norm\",\"img_norm\"], keep=False)\n",
    "\n",
    "text_equal_rows  = text_equal_rows = fff.duplicated([\"text_norm\"], keep=False)\n",
    " #fff.duplicated([\"text_norm\"], keep=False)  & ~mask_empty_text\n",
    "image_equal_rows = fff.duplicated([\"img_norm\"],  keep=False)  & ~mask_no_image\n",
    "\n",
    "# --- Disjunkte Aufteilung (keine Überschneidungen) ---\n",
    "mixed_rows       = (text_equal_rows & image_equal_rows) & ~exact_rows\n",
    "text_only_rows   = (text_equal_rows & ~image_equal_rows) & ~exact_rows\n",
    "image_only_rows  = (image_equal_rows & ~text_equal_rows) & ~exact_rows\n",
    "\n",
    "# exact_rows bleibt separat (vollständig identische Zeilen)\n",
    "\n",
    "# --- Prüfen, dass alles disjunkt ist ---\n",
    "assert not any([\n",
    "    (text_only_rows & image_only_rows).any(),\n",
    "    (text_only_rows & mixed_rows).any(),\n",
    "    (image_only_rows & mixed_rows).any(),\n",
    "    (exact_rows & (text_only_rows | image_only_rows | mixed_rows)).any()\n",
    "])\n",
    "\n",
    "# --- Eindeutig einzigartige Zeilen ---\n",
    "unique_rows = ~(exact_rows | text_only_rows | image_only_rows | mixed_rows)\n",
    "\n",
    "# --- Kennzahlen ---\n",
    "print(\"== Übersicht ==\")\n",
    "print(f\"Gleicher Text (ALLE, inkl. exakter):  {text_equal_rows.sum()}\")\n",
    "print(f\"Gleiche Bilder (ALLE, inkl. exakter): {image_equal_rows.sum()}\")\n",
    "print(f\"Exakte Duplikat-Zeilen:               {exact_rows.sum()}\")\n",
    "print(\"Exakte Duplikate mit leerem Text:\", (exact_rows & mask_empty_text).sum())\n",
    "print(\"Exakte Duplikate mit echtem Text:\", (exact_rows & ~mask_empty_text).sum())\n",
    "print(f\"Nur Text-Duplikate (nicht exakt):     {text_only_rows.sum()}\")\n",
    "print(f\"Nur Bild-Duplikate (nicht exakt):     {image_only_rows.sum()}\")\n",
    "print(f\"Text+Bild dupl. (nicht exakt):        {mixed_rows.sum()}\")\n",
    "print(f\"Echte einzigartige Zeilen:            {unique_rows.sum()}\")\n",
    "print(f\"Gesamt:                               {len(fff)}\")\n",
    "\n",
    "# --- Exporte ---\n",
    "def save_groups(df, mask, group_cols, filename):\n",
    "    subset = df[mask].copy()\n",
    "    if subset.empty:\n",
    "        print(f\"Keine Gruppen für {filename}\")\n",
    "        return\n",
    "    subset[\"dupe_group\"] = subset.groupby(group_cols).ngroup()\n",
    "    subset.sort_values(group_cols + [\"date\",\"time\"], inplace=True)\n",
    "    subset.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Gespeichert: {filename} ({len(subset)} Zeilen)\")\n",
    "\n",
    "# Gruppenreports:\n",
    "# 1) „Gleicher Text (ALLE)“ – das ist die Menge, die du intuitiv meinst\n",
    "save_groups(fff, text_equal_rows, [\"text_norm\"], \"groups_text_all.csv\")\n",
    "\n",
    "# 2) „Nur Text-Duplikate, nicht exakt“\n",
    "save_groups(fff, text_only_rows, [\"text_norm\"], \"groups_text_only.csv\")\n",
    "\n",
    "# 3) Exakt\n",
    "save_groups(fff, exact_rows, [\"author_norm\",\"platform\",\"date\",\"time\",\"text_norm\",\"img_norm\"], \"groups_exact.csv\")\n",
    "\n",
    "# 4) Bilder\n",
    "save_groups(fff, image_equal_rows, [\"img_norm\"], \"groups_images_all.csv\")\n",
    "save_groups(fff, image_only_rows, [\"img_norm\"], \"groups_images_only.csv\")\n",
    "\n",
    "# 5) Mixed (gleicher Text & gleiches Bild, aber nicht komplett identisch)\n",
    "save_groups(fff, mixed_rows, [\"text_norm\",\"img_norm\"], \"groups_mixed.csv\")\n",
    "\n",
    "# Echte Uniques\n",
    "fff[unique_rows].to_csv(\"unique_posts.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Gespeichert: unique_posts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "892f054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autor: Donald Trump @realDonaldTrump\n",
      "Plattform: Twitter\n",
      "Datum: 2021-01-06\n",
      "Zeit: 15:45\n",
      "Text: This is a test post with multiple lines and even more text. As obviously, this is also part of the text. And what about this? @you Look here! Views: 123,456\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_metadata_text(text: str):\n",
    "    lines = [line.strip() for line in text.strip().splitlines() if line.strip()]\n",
    "    author_name = \"\"\n",
    "    handle = \"\"\n",
    "    platform = \"\"\n",
    "    date_str = \"\"\n",
    "    time_str = \"\"\n",
    "    content_lines = []\n",
    "\n",
    "    if len(lines) >= 2:\n",
    "        author_name = lines[0].strip()\n",
    "        match = re.search(\n",
    "            r\"(@[\\w]+)\\s*[•\\-]\\s*(.*?)\\s*[•\\-]\\s*([A-Za-z]+ \\d{1,2}, \\d{4})\\s*@\\s*(\\d{1,2}:\\d{2} [AP]M)\",\n",
    "            lines[1]\n",
    "        )\n",
    "        if match:\n",
    "            handle = match.group(1).strip()\n",
    "            platform = match.group(2).strip()\n",
    "            date_str = match.group(3).strip()\n",
    "            time_str = match.group(4).strip()\n",
    "\n",
    "        start_idx = 2\n",
    "        if len(lines) > 2 and lines[2].startswith(\"View\"):  # z.B. \"View on ...\"\n",
    "            start_idx = 3\n",
    "        content_lines = lines[start_idx:]\n",
    "\n",
    "    # Absatzmarker durch Leerzeichen ersetzen\n",
    "    content_text = \" \".join(content_lines).strip()\n",
    "\n",
    "    # Doppelte Leerzeichen normalisieren\n",
    "    content_text = re.sub(r\"\\s{2,}\", \" \", content_text)\n",
    "\n",
    "    try:\n",
    "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%B %d, %Y %I:%M %p\")\n",
    "        return {\n",
    "            \"author\": f\"{author_name} {handle}\".strip(),\n",
    "            \"platform\": platform,\n",
    "            \"date\": dt.strftime(\"%Y-%m-%d\"),\n",
    "            \"time\": dt.strftime(\"%H:%M\"),\n",
    "            \"year\": int(dt.year),\n",
    "            \"month\": dt.strftime(\"%B\"),\n",
    "            \"day\": int(dt.day),\n",
    "            \"text\": content_text\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"author\": f\"{author_name} {handle}\".strip(),\n",
    "            \"platform\": platform,\n",
    "            \"date\": date_str,\n",
    "            \"time\": time_str,\n",
    "            \"year\": \"\",\n",
    "            \"month\": \"\",\n",
    "            \"day\": \"\",\n",
    "            \"text\": content_text\n",
    "        }\n",
    "\n",
    "# --- Testfunktion ---\n",
    "def test_extract_metadata():\n",
    "    sample_post = \"\"\"\n",
    "    Donald Trump\n",
    "    @realDonaldTrump • Twitter • January 6, 2021 @ 3:45 PM\n",
    "    View on Twitter\n",
    "    This is a test post\n",
    "    with multiple lines\n",
    "    and even more text.\n",
    "    \n",
    "    As obviously, this is also part of the text.\n",
    "    \n",
    "    And what about this? @you\n",
    "    Look here!\n",
    "    Views: 123,456\n",
    "    \"\"\"\n",
    "\n",
    "    result = extract_metadata_text(sample_post)\n",
    "    print(\"Autor:\", result[\"author\"])\n",
    "    print(\"Plattform:\", result[\"platform\"])\n",
    "    print(\"Datum:\", result[\"date\"])\n",
    "    print(\"Zeit:\", result[\"time\"])\n",
    "    print(\"Text:\", result[\"text\"])\n",
    "\n",
    "# Testlauf\n",
    "if __name__ == \"__main__\":\n",
    "    test_extract_metadata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "async def scrape_all_dynamic(max_posts=100000, max_no_new=5, output_file=\"factbase_posts_clean.csv\"):\n",
    "    posts_data = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    # 🔹 Vorhandene CSV einlesen, falls vorhanden\n",
    "    if os.path.exists(output_file):\n",
    "        df_existing = pd.read_csv(output_file)\n",
    "        print(f\"{len(df_existing)} Posts bereits in {output_file} gefunden.\")\n",
    "        for _, row in df_existing.iterrows():\n",
    "            key = f\"{row['author']}|{row['date']}|{row['time']}|{row['text']}\"\n",
    "            pid = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "            seen_ids.add(pid)\n",
    "        posts_data = df_existing.to_dict(\"records\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.firefox.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(\"https://rollcall.com/factbase-twitter/?platform=all&sort=date&sort_order=desc\")\n",
    "\n",
    "        no_new_rounds = 0\n",
    "        while True:\n",
    "            try:\n",
    "                await page.wait_for_selector(\"div.block\", timeout=15000)\n",
    "            except:\n",
    "                print(\"Timeout beim Warten auf Posts – Abbruch.\")\n",
    "                break\n",
    "\n",
    "            blocks = await page.query_selector_all(\"div.block\")\n",
    "            print(f\"Aktuell {len(blocks)} Posts im DOM\")\n",
    "\n",
    "            new_in_round = 0\n",
    "            for block in blocks:\n",
    "                text = (await block.inner_text()).strip()\n",
    "                data = extract_metadata_text(text)\n",
    "                pid = make_post_id(data)\n",
    "\n",
    "                if pid not in seen_ids:\n",
    "                    seen_ids.add(pid)\n",
    "                    posts_data.append(data)\n",
    "                    new_in_round += 1\n",
    "\n",
    "            print(f\"Neue eindeutige Posts in dieser Runde: {new_in_round}\")\n",
    "\n",
    "            if new_in_round == 0:\n",
    "                no_new_rounds += 1\n",
    "                if no_new_rounds >= max_no_new:\n",
    "                    print(\"Mehrfach keine neuen Posts gefunden – beende Scraping.\")\n",
    "                    break\n",
    "            else:\n",
    "                no_new_rounds = 0\n",
    "\n",
    "            print(f\"Gesamt bisher gespeichert: {len(posts_data)}\")\n",
    "\n",
    "            if len(posts_data) >= max_posts:\n",
    "                print(\"Maximale Anzahl Posts erreicht.\")\n",
    "                break\n",
    "\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(2000)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # 🔹 Kombinierte Daten abspeichern\n",
    "    df = pd.DataFrame(posts_data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Scraping abgeschlossen. Gesamt: {len(df)} eindeutige Posts gespeichert in {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
