{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "038b13d9",
   "metadata": {},
   "source": [
    "# 02 Vorverarbeitung_Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ae900",
   "metadata": {},
   "source": [
    "## Tagger f√ºr POS, Lemma,.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f38dd",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427642f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (6.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.13.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f584aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Donald PROPN\n",
      "Trump Trump PROPN\n",
      "posted post VERB\n",
      "a a DET\n",
      "new new ADJ\n",
      "tweet tweet NOUN\n",
      ". . PUNCT\n",
      "@realdonaldtrump @realdonaldtrump X\n",
      "! ! PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Donald Trump posted a new tweet. @realdonaldtrump!\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc03105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478c602c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     post_id                                               word  \\\n",
      "300       17                                                  ,   \n",
      "301       17                                                300   \n",
      "302       17                                            BILLION   \n",
      "303       17                                            DOLLARS   \n",
      "304       17                                                  !   \n",
      "305       18  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "306       19  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "307       20  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "308       21  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "309       22  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "310       23  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "311       24  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "312       25  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "313       26  https://truthsocial.com/@realDonaldTrump/11499...   \n",
      "314       26                                           STRONGER   \n",
      "315       26                                               THAN   \n",
      "316       26                                               EVER   \n",
      "317       26                                                  !   \n",
      "318       26                                                  !   \n",
      "319       26                                                  !   \n",
      "320       27  https://truthsocial.com/@realDonaldTrump/11498...   \n",
      "321       28  https://truthsocial.com/@realDonaldTrump/11498...   \n",
      "322       29  https://www.whitehouse.gov/articles/2025/08/pr...   \n",
      "323       30                                                For   \n",
      "324       30                                               more   \n",
      "325       30                                               than   \n",
      "326       30                                                 35   \n",
      "327       30                                              years   \n",
      "328       30                                                  ,   \n",
      "329       30                                            Armenia   \n",
      "330       30                                                and   \n",
      "331       30                                         Azerbaijan   \n",
      "332       30                                               have   \n",
      "333       30                                             fought   \n",
      "334       30                                                  a   \n",
      "335       30                                             bitter   \n",
      "336       30                                           conflict   \n",
      "337       30                                               that   \n",
      "338       30                                           resulted   \n",
      "339       30                                                 in   \n",
      "340       30                                         tremendous   \n",
      "341       30                                          suffering   \n",
      "342       30                                                ...   \n",
      "343       30                                               many   \n",
      "344       30                                              tried   \n",
      "345       30                                                 to   \n",
      "346       30                                               find   \n",
      "347       30                                                  a   \n",
      "348       30                                         resolution   \n",
      "349       30                                                ...   \n",
      "350       30                                                and   \n",
      "351       30                                               they   \n",
      "352       30                                               were   \n",
      "353       30                                       unsuccessful   \n",
      "354       30                                                  .   \n",
      "\n",
      "                                                 lemma    pos  \\\n",
      "300                                                  ,  PUNCT   \n",
      "301                                                300    NUM   \n",
      "302                                            billion    NUM   \n",
      "303                                             dollar   NOUN   \n",
      "304                                                  !  PUNCT   \n",
      "305  https://truthsocial.com/@realdonaldtrump/11499...    NUM   \n",
      "306  https://truthsocial.com/@realdonaldtrump/11499...    NUM   \n",
      "307  https://truthsocial.com/@realdonaldtrump/11499...    NUM   \n",
      "308  https://truthsocial.com/@realdonaldtrump/11499...    NUM   \n",
      "309  https://truthsocial.com/@realDonaldTrump/11499...   NOUN   \n",
      "310  https://truthsocial.com/@realDonaldTrump/11499...   NOUN   \n",
      "311  https://truthsocial.com/@realdonaldtrump/11499...    NUM   \n",
      "312  https://truthsocial.com/@realdonaldtrump/11499...    NUM   \n",
      "313  https://truthsocial.com/@realdonaldtrump/11499...    ADV   \n",
      "314                                             strong    ADJ   \n",
      "315                                               than    ADP   \n",
      "316                                               ever    ADV   \n",
      "317                                                  !  PUNCT   \n",
      "318                                                  !  PUNCT   \n",
      "319                                                  !  PUNCT   \n",
      "320  https://truthsocial.com/@realDonaldTrump/11498...   NOUN   \n",
      "321  https://truthsocial.com/@realdonaldtrump/11498...    NUM   \n",
      "322  https://www.whitehouse.gov/articles/2025/08/pr...  PROPN   \n",
      "323                                                for    ADP   \n",
      "324                                               more    ADJ   \n",
      "325                                               than    ADP   \n",
      "326                                                 35    NUM   \n",
      "327                                               year   NOUN   \n",
      "328                                                  ,  PUNCT   \n",
      "329                                            Armenia  PROPN   \n",
      "330                                                and  CCONJ   \n",
      "331                                         Azerbaijan  PROPN   \n",
      "332                                               have    AUX   \n",
      "333                                              fight   VERB   \n",
      "334                                                  a    DET   \n",
      "335                                             bitter    ADJ   \n",
      "336                                           conflict   NOUN   \n",
      "337                                               that   PRON   \n",
      "338                                             result   VERB   \n",
      "339                                                 in    ADP   \n",
      "340                                         tremendous    ADJ   \n",
      "341                                          suffering   NOUN   \n",
      "342                                                ...  PUNCT   \n",
      "343                                               many    ADJ   \n",
      "344                                                try   VERB   \n",
      "345                                                 to   PART   \n",
      "346                                               find   VERB   \n",
      "347                                                  a    DET   \n",
      "348                                         resolution   NOUN   \n",
      "349                                                ...  PUNCT   \n",
      "350                                                and  CCONJ   \n",
      "351                                               they   PRON   \n",
      "352                                                 be    AUX   \n",
      "353                                       unsuccessful    ADJ   \n",
      "354                                                  .  PUNCT   \n",
      "\n",
      "                                               lemma_p  \n",
      "300                                            ,_PUNCT  \n",
      "301                                            300_NUM  \n",
      "302                                        billion_NUM  \n",
      "303                                        dollar_NOUN  \n",
      "304                                            !_PUNCT  \n",
      "305  https://truthsocial.com/@realdonaldtrump/11499...  \n",
      "306  https://truthsocial.com/@realdonaldtrump/11499...  \n",
      "307  https://truthsocial.com/@realdonaldtrump/11499...  \n",
      "308  https://truthsocial.com/@realdonaldtrump/11499...  \n",
      "309  https://truthsocial.com/@realDonaldTrump/11499...  \n",
      "310  https://truthsocial.com/@realDonaldTrump/11499...  \n",
      "311  https://truthsocial.com/@realdonaldtrump/11499...  \n",
      "312  https://truthsocial.com/@realdonaldtrump/11499...  \n",
      "313  https://truthsocial.com/@realdonaldtrump/11499...  \n",
      "314                                         strong_ADJ  \n",
      "315                                           than_ADP  \n",
      "316                                           ever_ADV  \n",
      "317                                            !_PUNCT  \n",
      "318                                            !_PUNCT  \n",
      "319                                            !_PUNCT  \n",
      "320  https://truthsocial.com/@realDonaldTrump/11498...  \n",
      "321  https://truthsocial.com/@realdonaldtrump/11498...  \n",
      "322  https://www.whitehouse.gov/articles/2025/08/pr...  \n",
      "323                                            for_ADP  \n",
      "324                                           more_ADJ  \n",
      "325                                           than_ADP  \n",
      "326                                             35_NUM  \n",
      "327                                          year_NOUN  \n",
      "328                                            ,_PUNCT  \n",
      "329                                      Armenia_PROPN  \n",
      "330                                          and_CCONJ  \n",
      "331                                   Azerbaijan_PROPN  \n",
      "332                                           have_AUX  \n",
      "333                                         fight_VERB  \n",
      "334                                              a_DET  \n",
      "335                                         bitter_ADJ  \n",
      "336                                      conflict_NOUN  \n",
      "337                                          that_PRON  \n",
      "338                                        result_VERB  \n",
      "339                                             in_ADP  \n",
      "340                                     tremendous_ADJ  \n",
      "341                                     suffering_NOUN  \n",
      "342                                          ..._PUNCT  \n",
      "343                                           many_ADJ  \n",
      "344                                           try_VERB  \n",
      "345                                            to_PART  \n",
      "346                                          find_VERB  \n",
      "347                                              a_DET  \n",
      "348                                    resolution_NOUN  \n",
      "349                                          ..._PUNCT  \n",
      "350                                          and_CCONJ  \n",
      "351                                          they_PRON  \n",
      "352                                             be_AUX  \n",
      "353                                   unsuccessful_ADJ  \n",
      "354                                            ._PUNCT  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# Englisches Modell laden (ggf. herunterladen: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ergebnis-Container\n",
    "all_results = []\n",
    "\n",
    "for idx, text in enumerate(df[\"text\"], start=1):\n",
    "    if pd.isna(text):\n",
    "        continue  # leere Zellen √ºberspringen\n",
    "    doc = nlp(str(text))\n",
    "    for token in doc:\n",
    "        all_results.append({\n",
    "            \"post_id\": idx,\n",
    "            \"word\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"pos\": token.pos_,\n",
    "            \"lemma_p\": f\"{token.lemma_}_{token.pos_}\"\n",
    "        })\n",
    "\n",
    "# In DataFrame umwandeln\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Optional: speichern\n",
    "pos_df.to_csv(\"factbase_posts_pos_tags.csv\", index=False)\n",
    "\n",
    "print(pos_df[300:355])\n",
    "# der Code funktioniert; allerdings werden Emojis, @ und Post-spezifische Dinge nicht erkannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31aaceb4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     post_id                         author      platform        date   time  \\\n",
      "200        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "201        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "202        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "203        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "204        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "205        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "206        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "207        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "208        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "209        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "210        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "211        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "212        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "213        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "214        8  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  19:39   \n",
      "\n",
      "     day   month    year  sentence_id  token_id  \\\n",
      "200  8.0  August  2025.0            1         2   \n",
      "201  8.0  August  2025.0            1         3   \n",
      "202  8.0  August  2025.0            1         4   \n",
      "203  8.0  August  2025.0            1         5   \n",
      "204  8.0  August  2025.0            1         6   \n",
      "205  8.0  August  2025.0            1         7   \n",
      "206  8.0  August  2025.0            1         8   \n",
      "207  8.0  August  2025.0            1         9   \n",
      "208  8.0  August  2025.0            1        10   \n",
      "209  8.0  August  2025.0            1        11   \n",
      "210  8.0  August  2025.0            1        12   \n",
      "211  8.0  August  2025.0            1        13   \n",
      "212  8.0  August  2025.0            1        14   \n",
      "213  8.0  August  2025.0            1        15   \n",
      "214  8.0  August  2025.0            1        16   \n",
      "\n",
      "                                         sentence_text       word      lemma  \\\n",
      "200  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...  President  President   \n",
      "201  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...      Trump      Trump   \n",
      "202  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...         ‚Äôs         ‚Äôs   \n",
      "203  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...    tariffs     tariff   \n",
      "204  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...        are         be   \n",
      "205  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...    WORKING       work   \n",
      "206  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...          ‚Äî          ‚Äî   \n",
      "207  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...          $          $   \n",
      "208  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...        152        152   \n",
      "209  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...    BILLION    billion   \n",
      "210  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...  collected    collect   \n",
      "211  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...      since      since   \n",
      "212  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...    January    January   \n",
      "213  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...          !          !   \n",
      "214  ‚ÄúPresident Trump‚Äôs tariffs are WORKING ‚Äî $152 ...          ‚Äù          \"   \n",
      "\n",
      "       pos          lemma_p  \n",
      "200  PROPN  President_PROPN  \n",
      "201  PROPN      Trump_PROPN  \n",
      "202   PART          ‚Äôs_PART  \n",
      "203   NOUN      tariff_NOUN  \n",
      "204    AUX           be_AUX  \n",
      "205   VERB        work_VERB  \n",
      "206  PUNCT          ‚Äî_PUNCT  \n",
      "207    SYM            $_SYM  \n",
      "208    NUM          152_NUM  \n",
      "209    NUM      billion_NUM  \n",
      "210   VERB     collect_VERB  \n",
      "211  SCONJ      since_SCONJ  \n",
      "212  PROPN    January_PROPN  \n",
      "213  PUNCT          !_PUNCT  \n",
      "214  PUNCT          \"_PUNCT  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fpt = pd.read_csv(\"factbase_posts_pos_tags.csv\")\n",
    "print(fpt[200:215])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1921862a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4127270, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a027762e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/yb/nh3fwp1s113dp86cld34qkhh0000gn/T/ipykernel_2117/1198815709.py\", line 2, in <module>\n",
      "    import spacy\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç POS-Tagging der Texte...\n",
      "‚úÖ POS-Tagging abgeschlossen. Ergebnis gespeichert in 'factbase_posts_tagged4.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Lade das englische spaCy-Modell\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# CSV-Datei einlesen (anpassen, falls du einen anderen Namen verwendet hast)\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# Neue Spalten vorbereiten\n",
    "all_tokens = []\n",
    "\n",
    "print(\"POS-Tagging der Texte...\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    doc = nlp(str(row['text']))\n",
    "    for token in doc:\n",
    "        if not token.is_space:\n",
    "            all_tokens.append({\n",
    "                \"id\": row[\"id\"],\n",
    "                \"author\": row[\"author\"],\n",
    "                \"platform\": row[\"platform\"],\n",
    "                \"date\": row[\"date\"],\n",
    "                \"day\": row[\"day\"],\n",
    "                \"month\": row[\"month\"],\n",
    "                \"year\": row[\"year\"],\n",
    "                \"time\": row[\"time\"],\n",
    "                \"text\": row[\"text\"],\n",
    "                \"word\": token.text,\n",
    "                \"lemma\": token.lemma_,\n",
    "                \"pos\": token.pos_,\n",
    "                \"lemma_p\": f\"{token.lemma_}_{token.pos_}\"\n",
    "            })\n",
    "\n",
    "# Neues DataFrame mit Tokens\n",
    "token_df = pd.DataFrame(all_tokens)\n",
    "\n",
    "# Als neue CSV-Datei speichern\n",
    "token_df.to_csv(\"factbase_posts_tagged4.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"POS-Tagging abgeschlossen. Ergebnis gespeichert in 'factbase_posts_tagged4.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "906859f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            author      platform        date  day   month  \\\n",
      "500  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "501  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "502  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "503  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "504  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "505  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "506  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "507  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "508  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "509  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "510  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "511  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "512  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "513  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "514  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "515  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "516  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "517  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "518  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "519  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "520  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "521  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "522  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "523  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "524  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "525  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "526  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "527  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "528  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "529  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "530  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "531  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "532  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "533  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "534  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "535  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "536  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "537  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "538  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "539  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "540  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "541  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "542  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "543  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "544  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "545  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "546  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "547  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "548  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "549  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "550  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "551  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "552  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "553  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "554  Donald Trump @realDonaldTrump  Truth Social  2025-08-07  7.0  August   \n",
      "\n",
      "       year   time                                               text  \\\n",
      "500  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "501  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "502  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "503  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "504  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "505  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "506  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "507  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "508  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "509  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "510  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "511  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "512  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "513  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "514  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "515  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "516  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "517  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "518  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "519  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "520  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "521  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "522  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "523  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "524  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "525  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "526  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "527  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "528  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "529  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "530  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "531  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "532  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "533  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "534  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "535  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "536  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "537  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "538  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "539  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "540  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "541  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "542  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "543  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "544  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "545  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "546  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "547  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "548  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "549  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "550  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "551  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "552  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "553  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "554  2025.0  16:19  The reason that The Wall Street Journal Editor...   \n",
      "\n",
      "             word         lemma    pos           lemma_p  \n",
      "500             ,             ,  PUNCT           ,_PUNCT  \n",
      "501           for           for    ADP           for_ADP  \n",
      "502       reasons        reason   NOUN       reason_NOUN  \n",
      "503       unknown       unknown    ADJ       unknown_ADJ  \n",
      "504             ,             ,  PUNCT           ,_PUNCT  \n",
      "505             ‚Äú             \"  PUNCT           \"_PUNCT  \n",
      "506           WIN           WIN  PROPN         WIN_PROPN  \n",
      "507             ,             ,  PUNCT           ,_PUNCT  \n",
      "508          BABY          BABY  PROPN        BABY_PROPN  \n",
      "509             ,             ,  PUNCT           ,_PUNCT  \n",
      "510           WIN           WIN  PROPN         WIN_PROPN  \n",
      "511             .             .  PUNCT           ._PUNCT  \n",
      "512             ‚Äù             \"  PUNCT           \"_PUNCT  \n",
      "513            If            if  SCONJ          if_SCONJ  \n",
      "514           the           the    DET           the_DET  \n",
      "515        United        United  PROPN      United_PROPN  \n",
      "516        States        States  PROPN      States_PROPN  \n",
      "517          were            be    AUX            be_AUX  \n",
      "518           not           not   PART          not_PART  \n",
      "519          able          able    ADJ          able_ADJ  \n",
      "520            to            to   PART           to_PART  \n",
      "521        charge        charge   VERB       charge_VERB  \n",
      "522       Tariffs        tariff   NOUN       tariff_NOUN  \n",
      "523            to            to    ADP            to_ADP  \n",
      "524         other         other    ADJ         other_ADJ  \n",
      "525     Countries       country   NOUN      country_NOUN  \n",
      "526             ,             ,  PUNCT           ,_PUNCT  \n",
      "527            it            it   PRON           it_PRON  \n",
      "528         would         would    AUX         would_AUX  \n",
      "529            be            be    AUX            be_AUX  \n",
      "530  Economically  economically    ADV  economically_ADV  \n",
      "531   defenseless   defenseless    ADJ   defenseless_ADJ  \n",
      "532           and           and  CCONJ         and_CCONJ  \n",
      "533             ,             ,  PUNCT           ,_PUNCT  \n",
      "534            of            of    ADP            of_ADP  \n",
      "535            no            no    DET            no_DET  \n",
      "536       further       further    ADJ       further_ADJ  \n",
      "537         force         force   NOUN        force_NOUN  \n",
      "538            or            or  CCONJ          or_CCONJ  \n",
      "539        effect        effect   NOUN       effect_NOUN  \n",
      "540             .             .  PUNCT           ._PUNCT  \n",
      "541           The           the    DET           the_DET  \n",
      "542          only          only    ADJ          only_ADJ  \n",
      "543         thing         thing   NOUN        thing_NOUN  \n",
      "544          that          that   PRON         that_PRON  \n",
      "545           can           can    AUX           can_AUX  \n",
      "546       destroy       destroy   VERB      destroy_VERB  \n",
      "547           our           our   PRON          our_PRON  \n",
      "548       Country       Country  PROPN     Country_PROPN  \n",
      "549           are            be    AUX            be_AUX  \n",
      "550       Crooked         crook   VERB        crook_VERB  \n",
      "551             ,             ,  PUNCT           ,_PUNCT  \n",
      "552       Radical       Radical  PROPN     Radical_PROPN  \n",
      "553          Left          Left  PROPN        Left_PROPN  \n",
      "554        Judges        Judges  PROPN      Judges_PROPN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fpt = pd.read_csv(\"factbase_posts_tagged4.csv\")\n",
    "print(fpt[500:555])\n",
    "#print(fpt[600:604].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c241687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpusanalyse in CQP-Web? Bequemer?\n",
    "# Spacy? Tagging damit? (basierend auf universal dependencies)\n",
    "# stanza (german universal dependencies treebank gsd, ud, hamburg dependency treebank)\n",
    "# stichprobenartig verschiedene Modelle testen\n",
    "# TweetNLP -> nur f√ºr Speechtagging geeignet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f2b0047",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     post_id                         author                 platform  \\\n",
      "500       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "501       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "502       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "503       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "504       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "505       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "506       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "507       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "508       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "509       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "510       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "511       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "512       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "513       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "514       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "515       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "516       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "517       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "518       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "519       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "520       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "521       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "522       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "523       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "524       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "525       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "526       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "527       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "528       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "529       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "530       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "531       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "532       36  Donald Trump @realDonaldTrump             Truth Social   \n",
      "533       37  Donald Trump @realDonaldTrump             Truth Social   \n",
      "534       37  Donald Trump @realDonaldTrump             Truth Social   \n",
      "535       37  Donald Trump @realDonaldTrump             Truth Social   \n",
      "536       37  Donald Trump @realDonaldTrump             Truth Social   \n",
      "537       37  Donald Trump @realDonaldTrump             Truth Social   \n",
      "538       37  Donald Trump @realDonaldTrump             Truth Social   \n",
      "539       37  Donald Trump @realDonaldTrump             Truth Social   \n",
      "540       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "541       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "542       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "543       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "544       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "545       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "546       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "547       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "548       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "549       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "550       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "551       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "552       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "553       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "554       38  Donald Trump @realDonaldTrump  Deleted ‚Ä¢  Truth Social   \n",
      "\n",
      "           date   time  day   month    year  sentence_id  token_id  \\\n",
      "500  2025-08-09  15:32  9.0  August  2025.0            2        11   \n",
      "501  2025-08-09  15:32  9.0  August  2025.0            2        12   \n",
      "502  2025-08-09  15:32  9.0  August  2025.0            2        13   \n",
      "503  2025-08-09  15:32  9.0  August  2025.0            2        14   \n",
      "504  2025-08-09  15:32  9.0  August  2025.0            2        15   \n",
      "505  2025-08-09  15:32  9.0  August  2025.0            2        16   \n",
      "506  2025-08-09  15:32  9.0  August  2025.0            2        17   \n",
      "507  2025-08-09  15:32  9.0  August  2025.0            2        18   \n",
      "508  2025-08-09  15:32  9.0  August  2025.0            2        19   \n",
      "509  2025-08-09  15:32  9.0  August  2025.0            2        20   \n",
      "510  2025-08-09  15:32  9.0  August  2025.0            2        21   \n",
      "511  2025-08-09  15:32  9.0  August  2025.0            2        22   \n",
      "512  2025-08-09  15:32  9.0  August  2025.0            2        23   \n",
      "513  2025-08-09  15:32  9.0  August  2025.0            2        24   \n",
      "514  2025-08-09  15:32  9.0  August  2025.0            2        25   \n",
      "515  2025-08-09  15:32  9.0  August  2025.0            2        26   \n",
      "516  2025-08-09  15:32  9.0  August  2025.0            2        27   \n",
      "517  2025-08-09  15:32  9.0  August  2025.0            2        28   \n",
      "518  2025-08-09  15:32  9.0  August  2025.0            3         1   \n",
      "519  2025-08-09  15:32  9.0  August  2025.0            3         2   \n",
      "520  2025-08-09  15:32  9.0  August  2025.0            3         3   \n",
      "521  2025-08-09  15:32  9.0  August  2025.0            3         4   \n",
      "522  2025-08-09  15:32  9.0  August  2025.0            3         5   \n",
      "523  2025-08-09  15:32  9.0  August  2025.0            3         6   \n",
      "524  2025-08-09  15:32  9.0  August  2025.0            3         7   \n",
      "525  2025-08-09  15:32  9.0  August  2025.0            3         8   \n",
      "526  2025-08-09  15:32  9.0  August  2025.0            3         9   \n",
      "527  2025-08-09  15:32  9.0  August  2025.0            3        10   \n",
      "528  2025-08-09  15:32  9.0  August  2025.0            3        11   \n",
      "529  2025-08-09  15:32  9.0  August  2025.0            3        12   \n",
      "530  2025-08-09  15:32  9.0  August  2025.0            4         1   \n",
      "531  2025-08-09  15:32  9.0  August  2025.0            4         2   \n",
      "532  2025-08-09  15:32  9.0  August  2025.0            4         3   \n",
      "533  2025-08-09  18:07  9.0  August  2025.0            1         1   \n",
      "534  2025-08-09  18:07  9.0  August  2025.0            1         2   \n",
      "535  2025-08-09  18:07  9.0  August  2025.0            1         3   \n",
      "536  2025-08-09  18:07  9.0  August  2025.0            1         4   \n",
      "537  2025-08-09  18:07  9.0  August  2025.0            1         5   \n",
      "538  2025-08-09  18:07  9.0  August  2025.0            1         6   \n",
      "539  2025-08-09  18:07  9.0  August  2025.0            1         7   \n",
      "540  2025-08-09  18:31  9.0  August  2025.0            1         1   \n",
      "541  2025-08-09  18:31  9.0  August  2025.0            1         2   \n",
      "542  2025-08-09  18:31  9.0  August  2025.0            1         3   \n",
      "543  2025-08-09  18:31  9.0  August  2025.0            1         4   \n",
      "544  2025-08-09  18:31  9.0  August  2025.0            1         5   \n",
      "545  2025-08-09  18:31  9.0  August  2025.0            1         6   \n",
      "546  2025-08-09  18:31  9.0  August  2025.0            1         7   \n",
      "547  2025-08-09  18:31  9.0  August  2025.0            1         8   \n",
      "548  2025-08-09  18:31  9.0  August  2025.0            1         9   \n",
      "549  2025-08-09  18:31  9.0  August  2025.0            1        10   \n",
      "550  2025-08-09  18:31  9.0  August  2025.0            1        11   \n",
      "551  2025-08-09  18:31  9.0  August  2025.0            1        12   \n",
      "552  2025-08-09  18:31  9.0  August  2025.0            1        13   \n",
      "553  2025-08-09  18:31  9.0  August  2025.0            1        14   \n",
      "554  2025-08-09  18:31  9.0  August  2025.0            1        15   \n",
      "\n",
      "                                         sentence_text             word  \\\n",
      "500  Since the beginning of my Second Term, Tammy h...             been   \n",
      "501  Since the beginning of my Second Term, Tammy h...          serving   \n",
      "502  Since the beginning of my Second Term, Tammy h...             with   \n",
      "503  Since the beginning of my Second Term, Tammy h...      distinction   \n",
      "504  Since the beginning of my Second Term, Tammy h...               as   \n",
      "505  Since the beginning of my Second Term, Tammy h...     Spokesperson   \n",
      "506  Since the beginning of my Second Term, Tammy h...               of   \n",
      "507  Since the beginning of my Second Term, Tammy h...              the   \n",
      "508  Since the beginning of my Second Term, Tammy h...            State   \n",
      "509  Since the beginning of my Second Term, Tammy h...       Department   \n",
      "510  Since the beginning of my Second Term, Tammy h...                ,   \n",
      "511  Since the beginning of my Second Term, Tammy h...            where   \n",
      "512  Since the beginning of my Second Term, Tammy h...              she   \n",
      "513  Since the beginning of my Second Term, Tammy h...              did   \n",
      "514  Since the beginning of my Second Term, Tammy h...                a   \n",
      "515  Since the beginning of my Second Term, Tammy h...        fantastic   \n",
      "516  Since the beginning of my Second Term, Tammy h...              job   \n",
      "517  Since the beginning of my Second Term, Tammy h...                .   \n",
      "518  Tammy Bruce will represent our Country brillia...            Tammy   \n",
      "519  Tammy Bruce will represent our Country brillia...            Bruce   \n",
      "520  Tammy Bruce will represent our Country brillia...             will   \n",
      "521  Tammy Bruce will represent our Country brillia...        represent   \n",
      "522  Tammy Bruce will represent our Country brillia...              our   \n",
      "523  Tammy Bruce will represent our Country brillia...          Country   \n",
      "524  Tammy Bruce will represent our Country brillia...      brilliantly   \n",
      "525  Tammy Bruce will represent our Country brillia...               at   \n",
      "526  Tammy Bruce will represent our Country brillia...              the   \n",
      "527  Tammy Bruce will represent our Country brillia...           United   \n",
      "528  Tammy Bruce will represent our Country brillia...          Nations   \n",
      "529  Tammy Bruce will represent our Country brillia...                .   \n",
      "530                             Congratulations Tammy!  Congratulations   \n",
      "531                             Congratulations Tammy!            Tammy   \n",
      "532                             Congratulations Tammy!                !   \n",
      "533                   Last week in Aberdeen, Scotland.             Last   \n",
      "534                   Last week in Aberdeen, Scotland.             week   \n",
      "535                   Last week in Aberdeen, Scotland.               in   \n",
      "536                   Last week in Aberdeen, Scotland.         Aberdeen   \n",
      "537                   Last week in Aberdeen, Scotland.                ,   \n",
      "538                   Last week in Aberdeen, Scotland.         Scotland   \n",
      "539                   Last week in Aberdeen, Scotland.                .   \n",
      "540  Crooked Nancy Pelosi, and her very ‚Äúinterestin...          Crooked   \n",
      "541  Crooked Nancy Pelosi, and her very ‚Äúinterestin...            Nancy   \n",
      "542  Crooked Nancy Pelosi, and her very ‚Äúinterestin...           Pelosi   \n",
      "543  Crooked Nancy Pelosi, and her very ‚Äúinterestin...                ,   \n",
      "544  Crooked Nancy Pelosi, and her very ‚Äúinterestin...              and   \n",
      "545  Crooked Nancy Pelosi, and her very ‚Äúinterestin...              her   \n",
      "546  Crooked Nancy Pelosi, and her very ‚Äúinterestin...             very   \n",
      "547  Crooked Nancy Pelosi, and her very ‚Äúinterestin...                ‚Äú   \n",
      "548  Crooked Nancy Pelosi, and her very ‚Äúinterestin...      interesting   \n",
      "549  Crooked Nancy Pelosi, and her very ‚Äúinterestin...                ‚Äù   \n",
      "550  Crooked Nancy Pelosi, and her very ‚Äúinterestin...          husband   \n",
      "551  Crooked Nancy Pelosi, and her very ‚Äúinterestin...                ,   \n",
      "552  Crooked Nancy Pelosi, and her very ‚Äúinterestin...             beat   \n",
      "553  Crooked Nancy Pelosi, and her very ‚Äúinterestin...             ever   \n",
      "554  Crooked Nancy Pelosi, and her very ‚Äúinterestin...            Hedge   \n",
      "\n",
      "              lemma    pos              lemma_p  \n",
      "500              be    AUX               be_AUX  \n",
      "501           serve   VERB           serve_VERB  \n",
      "502            with    ADP             with_ADP  \n",
      "503     distinction   NOUN     distinction_NOUN  \n",
      "504              as    ADP               as_ADP  \n",
      "505    Spokesperson  PROPN   Spokesperson_PROPN  \n",
      "506              of    ADP               of_ADP  \n",
      "507             the    DET              the_DET  \n",
      "508           State  PROPN          State_PROPN  \n",
      "509      Department  PROPN     Department_PROPN  \n",
      "510               ,  PUNCT              ,_PUNCT  \n",
      "511           where  SCONJ          where_SCONJ  \n",
      "512             she   PRON             she_PRON  \n",
      "513              do   VERB              do_VERB  \n",
      "514               a    DET                a_DET  \n",
      "515       fantastic    ADJ        fantastic_ADJ  \n",
      "516             job   NOUN             job_NOUN  \n",
      "517               .  PUNCT              ._PUNCT  \n",
      "518           Tammy  PROPN          Tammy_PROPN  \n",
      "519           Bruce  PROPN          Bruce_PROPN  \n",
      "520            will    AUX             will_AUX  \n",
      "521       represent   VERB       represent_VERB  \n",
      "522             our   PRON             our_PRON  \n",
      "523         country   NOUN         country_NOUN  \n",
      "524     brilliantly    ADV      brilliantly_ADV  \n",
      "525              at    ADP               at_ADP  \n",
      "526             the    DET              the_DET  \n",
      "527          United  PROPN         United_PROPN  \n",
      "528         Nations  PROPN        Nations_PROPN  \n",
      "529               .  PUNCT              ._PUNCT  \n",
      "530  congratulation   NOUN  congratulation_NOUN  \n",
      "531           Tammy  PROPN          Tammy_PROPN  \n",
      "532               !  PUNCT              !_PUNCT  \n",
      "533            last    ADJ             last_ADJ  \n",
      "534            week   NOUN            week_NOUN  \n",
      "535              in    ADP               in_ADP  \n",
      "536        Aberdeen  PROPN       Aberdeen_PROPN  \n",
      "537               ,  PUNCT              ,_PUNCT  \n",
      "538        Scotland  PROPN       Scotland_PROPN  \n",
      "539               .  PUNCT              ._PUNCT  \n",
      "540         crooked    ADJ          crooked_ADJ  \n",
      "541           Nancy  PROPN          Nancy_PROPN  \n",
      "542          Pelosi  PROPN         Pelosi_PROPN  \n",
      "543               ,  PUNCT              ,_PUNCT  \n",
      "544             and  CCONJ            and_CCONJ  \n",
      "545             her   PRON             her_PRON  \n",
      "546            very    ADV             very_ADV  \n",
      "547               \"  PUNCT              \"_PUNCT  \n",
      "548     interesting    ADJ      interesting_ADJ  \n",
      "549               \"  PUNCT              \"_PUNCT  \n",
      "550         husband   NOUN         husband_NOUN  \n",
      "551               ,  PUNCT              ,_PUNCT  \n",
      "552            beat   VERB            beat_VERB  \n",
      "553            ever    ADV             ever_ADV  \n",
      "554           Hedge  PROPN          Hedge_PROPN  \n"
     ]
    }
   ],
   "source": [
    "##das hier sieht auch gut aus\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# Englisches Modell laden (ggf. installieren: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ergebnis-Container\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        continue  # leere Zellen √ºberspringen\n",
    "    \n",
    "    doc = nlp(str(text))\n",
    "    \n",
    "    for sent_id, sent in enumerate(doc.sents, start=1):\n",
    "        sent_text = sent.text  # gesamter Satz als String\n",
    "        for token_id, token in enumerate(sent, start=1):\n",
    "            all_results.append({\n",
    "                \"post_id\": idx + 1,  # damit IDs bei 1 starten\n",
    "                \"author\": row[\"author\"],\n",
    "                \"platform\": row[\"platform\"],\n",
    "                \"date\": row[\"date\"],\n",
    "                \"time\": row[\"time\"],\n",
    "                \"day\": row[\"day\"],\n",
    "                \"month\": row[\"month\"],\n",
    "                \"year\": row[\"year\"],\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token_id\": token_id,\n",
    "                \"sentence_text\": sent_text,\n",
    "                \"word\": token.text,\n",
    "                \"lemma\": token.lemma_,\n",
    "                \"pos\": token.pos_,\n",
    "                \"lemma_p\": f\"{token.lemma_}_{token.pos_}\"\n",
    "            })\n",
    "\n",
    "# DataFrame erstellen\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "\n",
    "# CSV speichern\n",
    "pos_df.to_csv(\"factbase_posts_pos_tags.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(pos_df[500:555])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e77a36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     post_id            word           lemma    pos               lemma_p\n",
      "500       36           Tammy           Tammy  PROPN           Tammy_PROPN\n",
      "501       36          Bruce,          Bruce,  PROPN          Bruce,_PROPN\n",
      "502       36               a               a    DET                 a_DET\n",
      "503       36           Great           great    ADJ             great_ADJ\n",
      "504       36        Patriot,        patriot,    ADJ          patriot,_ADJ\n",
      "505       36      Television      television   NOUN       television_NOUN\n",
      "506       36    Personality,    personality,   NOUN     personality,_NOUN\n",
      "507       36             and             and  CCONJ             and_CCONJ\n",
      "508       36     Bestselling       bestselle   VERB        bestselle_VERB\n",
      "509       36         Author,         author,   NOUN          author,_NOUN\n",
      "510       36              as              as    ADP                as_ADP\n",
      "511       36             our             our   PRON              our_PRON\n",
      "512       36            next            next    ADJ              next_ADJ\n",
      "513       36          Deputy          Deputy  PROPN          Deputy_PROPN\n",
      "514       36  Representative  Representative  PROPN  Representative_PROPN\n",
      "515       36              of              of    ADP                of_ADP\n",
      "516       36             the             the    DET               the_DET\n",
      "517       36          United          United  PROPN          United_PROPN\n",
      "518       36          States          States  PROPN          States_PROPN\n",
      "519       36              to              to    ADP                to_ADP\n",
      "520       36             the             the    DET               the_DET\n",
      "521       36          United          United  PROPN          United_PROPN\n",
      "522       36        Nations,        Nations,  PROPN        Nations,_PROPN\n",
      "523       36            with            with    ADP              with_ADP\n",
      "524       36             the             the    DET               the_DET\n",
      "525       36            rank            rank   NOUN             rank_NOUN\n",
      "526       36              of              of    ADP                of_ADP\n",
      "527       36     Ambassador.     Ambassador.  PROPN     Ambassador._PROPN\n",
      "528       36           Since           since  SCONJ           since_SCONJ\n",
      "529       36             the             the    DET               the_DET\n",
      "530       36       beginning       beginning   NOUN        beginning_NOUN\n",
      "531       36              of              of    ADP                of_ADP\n",
      "532       36              my              my   PRON               my_PRON\n",
      "533       36          Second          Second  PROPN          Second_PROPN\n",
      "534       36           Term,           Term,  PROPN           Term,_PROPN\n",
      "535       36           Tammy           Tammy  PROPN           Tammy_PROPN\n",
      "536       36             has            have    AUX              have_AUX\n",
      "537       36            been              be    AUX                be_AUX\n",
      "538       36         serving           serve   VERB            serve_VERB\n",
      "539       36            with            with    ADP              with_ADP\n",
      "540       36     distinction     distinction   NOUN      distinction_NOUN\n",
      "541       36              as              as    ADP                as_ADP\n",
      "542       36    Spokesperson    Spokesperson  PROPN    Spokesperson_PROPN\n",
      "543       36              of              of    ADP                of_ADP\n",
      "544       36             the             the    DET               the_DET\n",
      "545       36           State           State  PROPN           State_PROPN\n",
      "546       36     Department,     Department,  PROPN     Department,_PROPN\n",
      "547       36           where           where  SCONJ           where_SCONJ\n",
      "548       36             she             she   PRON              she_PRON\n",
      "549       36             did              do   VERB               do_VERB\n",
      "550       36               a               a    DET                 a_DET\n",
      "551       36       fantastic       fantastic   NOUN        fantastic_NOUN\n",
      "552       36            job.            job.      X                job._X\n",
      "553       36           Tammy           Tammy  PROPN           Tammy_PROPN\n",
      "554       36           Bruce           Bruce  PROPN           Bruce_PROPN\n"
     ]
    }
   ],
   "source": [
    "## nur die Structure-Tags\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import re\n",
    "\n",
    "def create_twitter_tokenizer(nlp):\n",
    "    # Erweiterte Infix-Regel f√ºr Hashtags und Mentions (z.B. #NLP, @user)\n",
    "    infix_re = spacy.util.compile_infix_regex(\n",
    "        nlp.Defaults.infixes + [r'(?<=\\w)[#@](?=\\w)']\n",
    "    )\n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# SpaCy Modell laden\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Twitter-angepassten Tokenizer setzen\n",
    "nlp.tokenizer = create_twitter_tokenizer(nlp)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, text in enumerate(df[\"text\"], start=1):\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "    doc = nlp(str(text))\n",
    "    for token in doc:\n",
    "        all_results.append({\n",
    "            \"post_id\": idx,\n",
    "            \"word\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"pos\": token.pos_,\n",
    "            \"lemma_p\": f\"{token.lemma_}_{token.pos_}\"\n",
    "        })\n",
    "\n",
    "# In DataFrame umwandeln und speichern\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "pos_df.to_csv(\"factbase_posts_pos_tags.csv\", index=False)\n",
    "\n",
    "print(pos_df[500:555])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c468e1d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     post_id                         author      platform        date   time  \\\n",
      "500       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "501       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "502       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "503       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "504       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "505       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "506       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "507       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "508       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "509       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "510       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "511       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "512       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "513       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "514       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "515       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "516       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "517       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "518       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "519       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "520       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "521       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "522       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "523       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "524       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "525       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "526       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "527       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "528       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "529       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "530       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "531       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "532       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "533       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "534       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "535       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "536       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "537       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "538       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "539       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "540       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "541       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "542       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "543       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "544       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "545       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "546       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "547       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "548       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "549       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "550       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "551       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "552       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "553       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "554       36  Donald Trump @realDonaldTrump  Truth Social  2025-08-09  15:32   \n",
      "\n",
      "     day   month    year                                               text  \\\n",
      "500  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "501  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "502  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "503  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "504  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "505  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "506  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "507  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "508  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "509  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "510  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "511  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "512  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "513  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "514  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "515  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "516  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "517  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "518  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "519  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "520  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "521  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "522  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "523  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "524  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "525  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "526  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "527  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "528  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "529  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "530  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "531  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "532  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "533  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "534  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "535  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "536  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "537  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "538  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "539  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "540  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "541  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "542  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "543  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "544  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "545  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "546  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "547  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "548  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "549  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "550  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "551  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "552  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "553  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "554  9.0  August  2025.0  I am pleased to announce that I am nominating ...   \n",
      "\n",
      "               word           lemma    pos               lemma_p  \n",
      "500           Tammy           Tammy  PROPN           Tammy_PROPN  \n",
      "501          Bruce,          Bruce,  PROPN          Bruce,_PROPN  \n",
      "502               a               a    DET                 a_DET  \n",
      "503           Great           great    ADJ             great_ADJ  \n",
      "504        Patriot,        patriot,    ADJ          patriot,_ADJ  \n",
      "505      Television      television   NOUN       television_NOUN  \n",
      "506    Personality,    personality,   NOUN     personality,_NOUN  \n",
      "507             and             and  CCONJ             and_CCONJ  \n",
      "508     Bestselling       bestselle   VERB        bestselle_VERB  \n",
      "509         Author,         author,   NOUN          author,_NOUN  \n",
      "510              as              as    ADP                as_ADP  \n",
      "511             our             our   PRON              our_PRON  \n",
      "512            next            next    ADJ              next_ADJ  \n",
      "513          Deputy          Deputy  PROPN          Deputy_PROPN  \n",
      "514  Representative  Representative  PROPN  Representative_PROPN  \n",
      "515              of              of    ADP                of_ADP  \n",
      "516             the             the    DET               the_DET  \n",
      "517          United          United  PROPN          United_PROPN  \n",
      "518          States          States  PROPN          States_PROPN  \n",
      "519              to              to    ADP                to_ADP  \n",
      "520             the             the    DET               the_DET  \n",
      "521          United          United  PROPN          United_PROPN  \n",
      "522        Nations,        Nations,  PROPN        Nations,_PROPN  \n",
      "523            with            with    ADP              with_ADP  \n",
      "524             the             the    DET               the_DET  \n",
      "525            rank            rank   NOUN             rank_NOUN  \n",
      "526              of              of    ADP                of_ADP  \n",
      "527     Ambassador.     Ambassador.  PROPN     Ambassador._PROPN  \n",
      "528           Since           since  SCONJ           since_SCONJ  \n",
      "529             the             the    DET               the_DET  \n",
      "530       beginning       beginning   NOUN        beginning_NOUN  \n",
      "531              of              of    ADP                of_ADP  \n",
      "532              my              my   PRON               my_PRON  \n",
      "533          Second          Second  PROPN          Second_PROPN  \n",
      "534           Term,           Term,  PROPN           Term,_PROPN  \n",
      "535           Tammy           Tammy  PROPN           Tammy_PROPN  \n",
      "536             has            have    AUX              have_AUX  \n",
      "537            been              be    AUX                be_AUX  \n",
      "538         serving           serve   VERB            serve_VERB  \n",
      "539            with            with    ADP              with_ADP  \n",
      "540     distinction     distinction   NOUN      distinction_NOUN  \n",
      "541              as              as    ADP                as_ADP  \n",
      "542    Spokesperson    Spokesperson  PROPN    Spokesperson_PROPN  \n",
      "543              of              of    ADP                of_ADP  \n",
      "544             the             the    DET               the_DET  \n",
      "545           State           State  PROPN           State_PROPN  \n",
      "546     Department,     Department,  PROPN     Department,_PROPN  \n",
      "547           where           where  SCONJ           where_SCONJ  \n",
      "548             she             she   PRON              she_PRON  \n",
      "549             did              do   VERB               do_VERB  \n",
      "550               a               a    DET                 a_DET  \n",
      "551       fantastic       fantastic   NOUN        fantastic_NOUN  \n",
      "552            job.            job.      X                job._X  \n",
      "553           Tammy           Tammy  PROPN           Tammy_PROPN  \n",
      "554           Bruce           Bruce  PROPN           Bruce_PROPN  \n"
     ]
    }
   ],
   "source": [
    "## Structure-Tags plus Metadaten\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "def create_twitter_tokenizer(nlp):\n",
    "    infix_re = spacy.util.compile_infix_regex(\n",
    "        nlp.Defaults.infixes + [r'(?<=\\w)[#@](?=\\w)']\n",
    "    )\n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# SpaCy Modell laden\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Twitter-angepassten Tokenizer setzen\n",
    "nlp.tokenizer = create_twitter_tokenizer(nlp)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "    doc = nlp(str(text))\n",
    "    for token in doc:\n",
    "        all_results.append({\n",
    "            \"post_id\": idx + 1,\n",
    "            \"author\": row[\"author\"],\n",
    "            \"platform\": row[\"platform\"],\n",
    "            \"date\": row[\"date\"],\n",
    "            \"time\": row[\"time\"],\n",
    "            \"day\": row[\"day\"],\n",
    "            \"month\": row[\"month\"],\n",
    "            \"year\": row[\"year\"],\n",
    "            \"text\": text,\n",
    "            \"word\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"pos\": token.pos_,\n",
    "            \"lemma_p\": f\"{token.lemma_}_{token.pos_}\"\n",
    "        })\n",
    "\n",
    "# In DataFrame umwandeln und speichern\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "pos_df.to_csv(\"factbase_posts_pos_tags_with_metadata.csv\", index=False)\n",
    "\n",
    "print(pos_df[500:555])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4845757",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id                         author      platform        date   time  \\\n",
      "0        1         THE SOURCE FOR NEWS ON          None        None   None   \n",
      "1        4  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  16:14   \n",
      "2        5  Donald Trump @realDonaldTrump  Truth Social  2025-08-08  16:18   \n",
      "\n",
      "   day   month    year                                               text  \\\n",
      "0  NaN    None     NaN  Contact Us\\nAdvertise\\nEvents\\nPrivacy\\nRC Job...   \n",
      "1  8.0  August  2025.0  As President, my highest aspiration is to brin...   \n",
      "2  8.0  August  2025.0  RT: https://truthsocial.com/users/realDonaldTr...   \n",
      "\n",
      "                                                word  \\\n",
      "0  [Contact, Us, \\n, Advertise, \\n, Events, \\n, P...   \n",
      "1  [As, President,, my, highest, aspiration, is, ...   \n",
      "2  [RT:, https://truthsocial.com, /, users, /, re...   \n",
      "\n",
      "                                               lemma  \\\n",
      "0  [contact, Us, \\n, Advertise, \\n, event, \\n, Pr...   \n",
      "1  [as, President,, my, high, aspiration, be, to,...   \n",
      "2  [rt:, https://truthsocial.com, /, user, /, rea...   \n",
      "\n",
      "                                                 pos  \\\n",
      "0  [VERB, PROPN, SPACE, PROPN, SPACE, NOUN, SPACE...   \n",
      "1  [ADP, PROPN, PRON, ADJ, NOUN, AUX, PART, VERB,...   \n",
      "2  [VERB, PROPN, SYM, NOUN, SYM, NOUN, SYM, NOUN,...   \n",
      "\n",
      "                                             lemma_p  \n",
      "0  [contact_VERB, Us_PROPN, \\n_SPACE, Advertise_P...  \n",
      "1  [as_ADP, President,_PROPN, my_PRON, high_ADJ, ...  \n",
      "2  [rt:_VERB, https://truthsocial.com_PROPN, /_SY...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DataFrame mit allen Tokens und Metadaten einlesen (oder aus vorherigem Schritt √ºbernehmen)\n",
    "pos_df = pd.read_csv(\"factbase_posts_pos_tags_with_metadata.csv\")\n",
    "\n",
    "# Spalten, die wir gruppieren wollen (Metadaten behalten wir aus der ersten Zeile pro Post)\n",
    "metadata_cols = [\"post_id\", \"author\", \"platform\", \"date\", \"time\", \"day\", \"month\", \"year\", \"text\"]\n",
    "\n",
    "# Token-Spalten als Listen aggregieren\n",
    "grouped = pos_df.groupby(\"post_id\").agg({\n",
    "    **{col: 'first' for col in metadata_cols if col != \"post_id\"},  # Metadaten aus erster Zeile\n",
    "    \"word\": list,\n",
    "    \"lemma\": list,\n",
    "    \"pos\": list,\n",
    "    \"lemma_p\": list\n",
    "}).reset_index()\n",
    "\n",
    "# Optional: speichern\n",
    "grouped.to_csv(\"factbase_posts_grouped_tokens.csv\", index=False)\n",
    "\n",
    "print(grouped.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b7490",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "# Stanza Pipeline f√ºr Englisch laden (nur Tokenization, Lemma, POS)\n",
    "stanza.download('en')  # Nur einmal ausf√ºhren, falls noch nicht installiert\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma', use_gpu=False)\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "    doc = nlp(str(text))\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            # Ein Token kann mehrere W√∂rter enthalten (multi-word tokens),\n",
    "            # deshalb nehmen wir das erste Wort f√ºr lemma und pos\n",
    "            word = token.text\n",
    "            word_info = token.words[0]  # Erstes Wort im Token\n",
    "            lemma = word_info.lemma\n",
    "            pos = word_info.upos\n",
    "            lemma_p = f\"{lemma}_{pos}\"\n",
    "            \n",
    "            all_results.append({\n",
    "                \"post_id\": idx + 1,\n",
    "                \"author\": row[\"author\"],\n",
    "                \"platform\": row[\"platform\"],\n",
    "                \"date\": row[\"date\"],\n",
    "                \"time\": row[\"time\"],\n",
    "                \"day\": row[\"day\"],\n",
    "                \"month\": row[\"month\"],\n",
    "                \"year\": row[\"year\"],\n",
    "                \"text\": text,\n",
    "                \"word\": word,\n",
    "                \"lemma\": lemma,\n",
    "                \"pos\": pos,\n",
    "                \"lemma_p\": lemma_p\n",
    "            })\n",
    "\n",
    "# In DataFrame umwandeln und speichern\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "pos_df.to_csv(\"factbase_posts_pos_tags_stanza.csv\", index=False)\n",
    "\n",
    "print(pos_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Stanza POS-Tag DataFrame einlesen (aus vorherigem Schritt oder direkt √ºbernehmen)\n",
    "pos_df = pd.read_csv(\"factbase_posts_pos_tags_stanza.csv\")\n",
    "\n",
    "# Metadaten-Spalten\n",
    "metadata_cols = [\"post_id\", \"author\", \"platform\", \"date\", \"time\", \"day\", \"month\", \"year\", \"text\"]\n",
    "\n",
    "# Gruppieren und Token-Spalten als Listen aggregieren\n",
    "grouped = pos_df.groupby(\"post_id\").agg({\n",
    "    **{col: 'first' for col in metadata_cols if col != \"post_id\"},\n",
    "    \"word\": list,\n",
    "    \"lemma\": list,\n",
    "    \"pos\": list,\n",
    "    \"lemma_p\": list\n",
    "}).reset_index()\n",
    "\n",
    "# Optional: in CSV speichern\n",
    "grouped.to_csv(\"factbase_posts_grouped_stanza_tokens.csv\", index=False)\n",
    "\n",
    "print(grouped.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20adff",
   "metadata": {},
   "source": [
    "# TweetNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec15f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweetnlp\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# TweetNLP POS-Tagger laden (hier nutzen wir das Modell 'pos' von TweetNLP)\n",
    "pos_model = tweetnlp.load_model(\"pos\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "    # POS-Tagging mit TweetNLP (liefert Liste von dicts mit 'token', 'pos', 'lemma' etc.)\n",
    "    tagged_tokens = pos_model.predict(str(text))\n",
    "    \n",
    "    for token_info in tagged_tokens:\n",
    "        word = token_info['token']\n",
    "        pos = token_info.get('pos', '')  # falls nicht vorhanden\n",
    "        lemma = token_info.get('lemma', word)  # falls nicht vorhanden, Wort selbst\n",
    "        lemma_p = f\"{lemma}_{pos}\"\n",
    "        \n",
    "        all_results.append({\n",
    "            \"post_id\": idx + 1,\n",
    "            \"author\": row[\"author\"],\n",
    "            \"platform\": row[\"platform\"],\n",
    "            \"date\": row[\"date\"],\n",
    "            \"time\": row[\"time\"],\n",
    "            \"day\": row[\"day\"],\n",
    "            \"month\": row[\"month\"],\n",
    "            \"year\": row[\"year\"],\n",
    "            \"text\": text,\n",
    "            \"word\": word,\n",
    "            \"lemma\": lemma,\n",
    "            \"pos\": pos,\n",
    "            \"lemma_p\": lemma_p\n",
    "        })\n",
    "\n",
    "# In DataFrame umwandeln\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Gruppieren wie bei Stanza: Listen pro Post\n",
    "metadata_cols = [\"post_id\", \"author\", \"platform\", \"date\", \"time\", \"day\", \"month\", \"year\", \"text\"]\n",
    "\n",
    "grouped = pos_df.groupby(\"post_id\").agg({\n",
    "    **{col: 'first' for col in metadata_cols if col != \"post_id\"},\n",
    "    \"word\": list,\n",
    "    \"lemma\": list,\n",
    "    \"pos\": list,\n",
    "    \"lemma_p\": list\n",
    "}).reset_index()\n",
    "\n",
    "# Optional speichern\n",
    "grouped.to_csv(\"factbase_posts_grouped_tweetnlp.csv\", index=False)\n",
    "\n",
    "print(grouped.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa535c",
   "metadata": {},
   "source": [
    "# Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# Flair POS-Tagger laden\n",
    "tagger = SequenceTagger.load(\"pos-fast\")\n",
    "\n",
    "# Ergebnisliste vorbereiten\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    if pd.isna(text):\n",
    "        continue  # √ºberspringen falls leer\n",
    "\n",
    "    sentence = Sentence(str(text))\n",
    "    tagger.predict(sentence)  # POS-Tagging durchf√ºhren\n",
    "\n",
    "    for token in sentence:\n",
    "        all_results.append({\n",
    "            \"post_id\": idx,\n",
    "            \"author\": row.get(\"author\"),\n",
    "            \"platform\": row.get(\"platform\"),\n",
    "            \"date\": row.get(\"date\"),\n",
    "            \"time\": row.get(\"time\"),\n",
    "            \"day\": row.get(\"day\"),\n",
    "            \"month\": row.get(\"month\"),\n",
    "            \"year\": row.get(\"year\"),\n",
    "            \"word\": token.text,\n",
    "            \"lemma\": token.text.lower(),  # Flair liefert kein Lemma, hier als Platzhalter\n",
    "            \"pos\": token.get_tag('pos').value,\n",
    "            \"lemma_p\": f\"{token.text.lower()}_{token.get_tag('pos').value}\"\n",
    "        })\n",
    "\n",
    "# In DataFrame umwandeln\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Ergebnis speichern\n",
    "pos_df.to_csv(\"factbase_posts_flair_pos_tags.csv\", index=False)\n",
    "\n",
    "print(pos_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec86415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import spacy\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# Flair POS-Tagger laden\n",
    "tagger = SequenceTagger.load(\"pos-fast\")\n",
    "\n",
    "# SpaCy englisches Modell laden f√ºr Lemmatisierung\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "\n",
    "    # SpaCy-Dokument f√ºr Lemmata\n",
    "    spacy_doc = nlp(str(text))\n",
    "\n",
    "    # Flair Sentence f√ºr POS\n",
    "    flair_sentence = Sentence(str(text))\n",
    "    tagger.predict(flair_sentence)\n",
    "\n",
    "    # Achtung: Flair und SpaCy tokenisieren unterschiedlich!\n",
    "    # Deshalb versuchen wir, Tokens zu matchen per Position, falls gleich viele Tokens\n",
    "    if len(flair_sentence) == len(spacy_doc):\n",
    "        for flair_token, spacy_token in zip(flair_sentence, spacy_doc):\n",
    "            all_results.append({\n",
    "                \"post_id\": idx,\n",
    "                \"author\": row.get(\"author\"),\n",
    "                \"platform\": row.get(\"platform\"),\n",
    "                \"date\": row.get(\"date\"),\n",
    "                \"time\": row.get(\"time\"),\n",
    "                \"day\": row.get(\"day\"),\n",
    "                \"month\": row.get(\"month\"),\n",
    "                \"year\": row.get(\"year\"),\n",
    "                \"word\": flair_token.text,\n",
    "                \"lemma\": spacy_token.lemma_,\n",
    "                \"pos\": flair_token.get_tag('pos').value,\n",
    "                \"lemma_p\": f\"{spacy_token.lemma_}_{flair_token.get_tag('pos').value}\"\n",
    "            })\n",
    "    else:\n",
    "        # Falls Tokenanzahl nicht √ºbereinstimmt: fallback nur Flair POS + Wort, Lemma = Wort klein\n",
    "        for flair_token in flair_sentence:\n",
    "            all_results.append({\n",
    "                \"post_id\": idx,\n",
    "                \"author\": row.get(\"author\"),\n",
    "                \"platform\": row.get(\"platform\"),\n",
    "                \"date\": row.get(\"date\"),\n",
    "                \"time\": row.get(\"time\"),\n",
    "                \"day\": row.get(\"day\"),\n",
    "                \"month\": row.get(\"month\"),\n",
    "                \"year\": row.get(\"year\"),\n",
    "                \"word\": flair_token.text,\n",
    "                \"lemma\": flair_token.text.lower(),\n",
    "                \"pos\": flair_token.get_tag('pos').value,\n",
    "                \"lemma_p\": f\"{flair_token.text.lower()}_{flair_token.get_tag('pos').value}\"\n",
    "            })\n",
    "\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "pos_df.to_csv(\"factbase_posts_flair_spacy_pos_lemma.csv\", index=False)\n",
    "\n",
    "print(pos_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b769c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import spacy\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# Flair POS-Tagger laden\n",
    "tagger = SequenceTagger.load(\"pos-fast\")\n",
    "\n",
    "# SpaCy englisches Modell laden f√ºr Lemmatisierung\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_spacy_token_by_offset(spacy_tokens, start, end):\n",
    "    # Suche SpaCy Token, der mindestens teilweise im Bereich [start, end) liegt\n",
    "    for token in spacy_tokens:\n",
    "        token_start = token.idx\n",
    "        token_end = token.idx + len(token.text)\n",
    "        # Check f√ºr √úberlappung\n",
    "        if token_start <= end and token_end >= start:\n",
    "            return token\n",
    "    return None\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "\n",
    "    spacy_doc = nlp(str(text))\n",
    "    flair_sentence = Sentence(str(text))\n",
    "    tagger.predict(flair_sentence)\n",
    "\n",
    "    for flair_token in flair_sentence:\n",
    "        start = flair_token.start_pos\n",
    "        end = flair_token.end_pos\n",
    "        spacy_token = get_spacy_token_by_offset(spacy_doc, start, end)\n",
    "        if spacy_token:\n",
    "            lemma = spacy_token.lemma_\n",
    "        else:\n",
    "            # Falls kein passender Token gefunden wurde, fallback\n",
    "            lemma = flair_token.text.lower()\n",
    "\n",
    "        all_results.append({\n",
    "            \"post_id\": idx,\n",
    "            \"author\": row.get(\"author\"),\n",
    "            \"platform\": row.get(\"platform\"),\n",
    "            \"date\": row.get(\"date\"),\n",
    "            \"time\": row.get(\"time\"),\n",
    "            \"day\": row.get(\"day\"),\n",
    "            \"month\": row.get(\"month\"),\n",
    "            \"year\": row.get(\"year\"),\n",
    "            \"word\": flair_token.text,\n",
    "            \"lemma\": lemma,\n",
    "            \"pos\": flair_token.get_tag('pos').value,\n",
    "            \"lemma_p\": f\"{lemma}_{flair_token.get_tag('pos').value}\"\n",
    "        })\n",
    "\n",
    "pos_df = pd.DataFrame(all_results)\n",
    "pos_df.to_csv(\"factbase_posts_flair_spacy_pos_lemma_matched.csv\", index=False)\n",
    "\n",
    "print(pos_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17088f5d",
   "metadata": {},
   "source": [
    "# Bert/HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "\n",
    "# Daten laden\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "# spaCy Modell laden f√ºr Lemma\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Hugging Face POS Tagging Modell laden (z.B. 'vblagoje/bert-english-uncased-finetuned-pos')\n",
    "model_name = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Pipeline f√ºr Token Classification (POS)\n",
    "pos_tagger = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "    \n",
    "    # spaCy Doc zum Lemmatisieren\n",
    "    spacy_doc = nlp(str(text))\n",
    "    \n",
    "    # Hugging Face POS-Tagging auf Satz\n",
    "    hf_pos = pos_tagger(str(text))\n",
    "    \n",
    "    # spaCy Tokens und Offset f√ºr Matching vorbereiten\n",
    "    spacy_tokens = list(spacy_doc)\n",
    "    \n",
    "    # Funktion um spaCy-Token nach Offset zu suchen\n",
    "    def get_spacy_token_by_offset(start, end):\n",
    "        for token in spacy_tokens:\n",
    "            token_start = token.idx\n",
    "            token_end = token.idx + len(token.text)\n",
    "            if token_start <= end and token_end >= start:\n",
    "                return token\n",
    "        return None\n",
    "    \n",
    "    for entity in hf_pos:\n",
    "        word = entity['word']\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        pos_tag = entity['entity_group']  # z.B. 'NOUN', 'VERB'\n",
    "        \n",
    "        spacy_token = get_spacy_token_by_offset(start, end)\n",
    "        lemma = spacy_token.lemma_ if spacy_token else word.lower()\n",
    "        \n",
    "        results.append({\n",
    "            \"post_id\": idx,\n",
    "            \"author\": row.get(\"author\"),\n",
    "            \"platform\": row.get(\"platform\"),\n",
    "            \"date\": row.get(\"date\"),\n",
    "            \"time\": row.get(\"time\"),\n",
    "            \"day\": row.get(\"day\"),\n",
    "            \"month\": row.get(\"month\"),\n",
    "            \"year\": row.get(\"year\"),\n",
    "            \"word\": word,\n",
    "            \"lemma\": lemma,\n",
    "            \"pos\": pos_tag,\n",
    "            \"lemma_p\": f\"{lemma}_{pos_tag}\"\n",
    "        })\n",
    "\n",
    "pos_df = pd.DataFrame(results)\n",
    "pos_df.to_csv(\"factbase_posts_hf_pos_lemma.csv\", index=False)\n",
    "\n",
    "print(pos_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohne Pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import spacy\n",
    "\n",
    "# Modellname (POS-Tagging)\n",
    "model_name = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "\n",
    "# Laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# spaCy laden f√ºr Lemma\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Label mapping (Index zu POS-Tag) aus Modell-Config holen\n",
    "id2label = model.config.id2label\n",
    "\n",
    "# Daten einlesen\n",
    "df = pd.read_csv(\"factbase_posts_playwright5.csv\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    if pd.isna(text):\n",
    "        continue\n",
    "    spacy_doc = nlp(str(text))\n",
    "    \n",
    "    # Tokenize mit R√ºckgabe der offsets (start/end Zeichen im Text)\n",
    "    encoding = tokenizer(str(text), return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    offset_mappings = encoding[\"offset_mapping\"][0]  # Batchsize=1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    \n",
    "    logits = output.logits  # shape [1, seq_len, num_labels]\n",
    "    predictions = torch.argmax(logits, dim=2)[0].tolist()  # indices der besten Labels\n",
    "    \n",
    "    # Iteriere Tokens (skip Special Tokens wie CLS, SEP)\n",
    "    for idx_token, pred_id in enumerate(predictions):\n",
    "        # offsets (start, end) des Tokens im Originaltext\n",
    "        start, end = offset_mappings[idx_token].tolist()\n",
    "        if start == end:  # special tokens (CLS, SEP) haben offset 0,0\n",
    "            continue\n",
    "        \n",
    "        word_text = text[start:end]\n",
    "        pos_tag = id2label[pred_id]\n",
    "        \n",
    "        # Lemma suchen: SpaCy Token mit passendem Offset\n",
    "        lemma = None\n",
    "        for token in spacy_doc:\n",
    "            token_start = token.idx\n",
    "            token_end = token.idx + len(token.text)\n",
    "            # check ob offsets sich √ºberlappen (f√ºr seltene F√§lle)\n",
    "            if start >= token_start and end <= token_end:\n",
    "                lemma = token.lemma_\n",
    "                break\n",
    "        if lemma is None:\n",
    "            lemma = word_text.lower()\n",
    "        \n",
    "        results.append({\n",
    "            \"post_id\": idx,\n",
    "            \"author\": row.get(\"author\"),\n",
    "            \"platform\": row.get(\"platform\"),\n",
    "            \"date\": row.get(\"date\"),\n",
    "            \"time\": row.get(\"time\"),\n",
    "            \"day\": row.get(\"day\"),\n",
    "            \"month\": row.get(\"month\"),\n",
    "            \"year\": row.get(\"year\"),\n",
    "            \"word\": word_text,\n",
    "            \"lemma\": lemma,\n",
    "            \"pos\": pos_tag,\n",
    "            \"lemma_p\": f\"{lemma}_{pos_tag}\"\n",
    "        })\n",
    "\n",
    "pos_df = pd.DataFrame(results)\n",
    "pos_df.to_csv(\"factbase_posts_hf_pos_lemma_manual.csv\", index=False)\n",
    "\n",
    "print(pos_df.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
